{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61b11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44fd169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Apps_and_Programs\\anaconda\\envs\\CNN_TF_GPU\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m2\u001b[0m)      â”‚            \u001b[38;5;34m56\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m2\u001b[0m)      â”‚            \u001b[38;5;34m38\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m2\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚           \u001b[38;5;34m330\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,000</span> (82.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,000\u001b[0m (82.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,000</span> (82.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,000\u001b[0m (82.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(2,(3, 3),strides=(1,1), activation='relu', padding='valid',input_shape=(28,28,3)),\n",
    "    keras.layers.Conv2D(2,(3, 3),strides=(1,1), activation='relu', padding='valid'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.save(\"model2.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c68614",
   "metadata": {},
   "source": [
    "### ModelGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dfe4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modelgen.py\n",
    "\n",
    "Unified ModelGen class:\n",
    "  - Stage 1: Keras -> software_model.json\n",
    "  - Stage 2: software_model.json -> hardware_model.json\n",
    "  - Stage 3: Generate Memory files (mif/csv)\n",
    "  - Stage 4: Build HDL (RTL Verilog files) using build_rtl_v5 logic\n",
    "\n",
    "\n",
    "Usage:\n",
    "from modelgen import ModelGen\n",
    "mg = ModelGen(\"model.keras\", out_dir=\"project_output\")\n",
    "mg.generate_software_json()\n",
    "mg.compile(data_width=16, fraction_bits=14, signed=1, guard_type=2)\n",
    "mg.gen_files(save_index=True)\n",
    "mg.build()\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Layer support and helpers\n",
    "# ----------------------------\n",
    "SUPPORTED_LAYERS = {\n",
    "    \"Conv2D\": {\n",
    "        \"supported\": [\"filters\", \"kernel_size\", \"strides\", \"activation\"],\n",
    "        \"valid_activations\": [\"relu\", None],\n",
    "    },\n",
    "    \"MaxPooling2D\": {\n",
    "        \"supported\": [\"pool_size\", \"strides\"]\n",
    "    },\n",
    "    \"Dense\": {\n",
    "        \"supported\": [\"units\", \"activation\"],\n",
    "        \"valid_activations\": [\"relu\", \"sigmoid\", \"softmax\"],\n",
    "    },\n",
    "    \"Flatten\": {\"supported\": []},\n",
    "}\n",
    "\n",
    "\n",
    "def _shape_to_list(shape):\n",
    "    if shape is None:\n",
    "        return None\n",
    "    return [int(s) if s is not None else None for s in shape]\n",
    "\n",
    "\n",
    "def _scalarize(param):\n",
    "    if isinstance(param, (list, tuple)):\n",
    "        if len(set(param)) == 1:\n",
    "            return param[0]\n",
    "        raise ValueError(f\"âš ï¸ Only square kernels / uniform strides supported, got {param}\")\n",
    "    return param\n",
    "\n",
    "\n",
    "def _infer_layer_shapes(model):\n",
    "    # Try to infer input shape\n",
    "    if hasattr(model, \"inputs\") and model.inputs:\n",
    "        input_shape = [1 if s is None else s for s in model.inputs[0].shape]\n",
    "    elif hasattr(model, \"input_shape\") and model.input_shape:\n",
    "        input_shape = [1 if s is None else s for s in model.input_shape]\n",
    "    else:\n",
    "        raise ValueError(\"âŒ Could not infer model input shape.\")\n",
    "\n",
    "    dummy_input = tf.zeros(input_shape)\n",
    "    x = dummy_input\n",
    "    layer_shapes = []\n",
    "    for layer in model.layers:\n",
    "        in_shape = _shape_to_list(x.shape)\n",
    "        x = layer(x)\n",
    "        out_shape = _shape_to_list(x.shape)\n",
    "        layer_shapes.append((in_shape, out_shape))\n",
    "    return layer_shapes\n",
    "\n",
    "\n",
    "def _extract_layer_info(layer, in_shape, out_shape):\n",
    "    ltype = layer.__class__.__name__\n",
    "    cfg = layer.get_config()\n",
    "    data = {\"name\": layer.name, \"type\": ltype}\n",
    "\n",
    "    # shapes\n",
    "    if in_shape is None:\n",
    "        raise ValueError(\"Input shape inference failed.\")\n",
    "    if len(in_shape) == 4:\n",
    "        _, in_h, in_w, in_c = in_shape\n",
    "        data[\"input_shape\"] = [in_h, in_w]\n",
    "        data[\"input_channels\"] = in_c\n",
    "    elif len(in_shape) == 2:\n",
    "        _, n = in_shape\n",
    "        data[\"input_shape\"] = [n]\n",
    "        # Preserve channel information for Flatten (inherit from previous layer)\n",
    "        if layer.__class__.__name__ == \"Flatten\" and hasattr(layer, '_inbound_nodes') and layer._inbound_nodes:\n",
    "            prev_layer = layer._inbound_nodes[0].inbound_layers\n",
    "            if not isinstance(prev_layer, list):\n",
    "                prev_layer = [prev_layer]\n",
    "            if prev_layer and hasattr(prev_layer[0], 'output_shape'):\n",
    "                prev_out_shape = _shape_to_list(prev_layer[0].output_shape)\n",
    "                if len(prev_out_shape) == 4:\n",
    "                    _, h, w, c = prev_out_shape\n",
    "                    data[\"input_channels\"] = c\n",
    "                else:\n",
    "                    data[\"input_channels\"] = 1\n",
    "            else:\n",
    "                data[\"input_channels\"] = 1\n",
    "        else:\n",
    "            data[\"input_channels\"] = 1\n",
    "\n",
    "\n",
    "    if len(out_shape) == 4:\n",
    "        _, out_h, out_w, out_c = out_shape\n",
    "        data[\"output_shape\"] = [out_h, out_w]\n",
    "        data[\"output_channels\"] = out_c\n",
    "    elif len(out_shape) == 2:\n",
    "        _, n = out_shape\n",
    "        data[\"output_shape\"] = [n]\n",
    "        data[\"output_channels\"] = n\n",
    "\n",
    "    # parameters\n",
    "    for key in SUPPORTED_LAYERS.get(ltype, {}).get(\"supported\", []):\n",
    "        if key in cfg:\n",
    "            val = cfg[key]\n",
    "            if key == \"strides\":\n",
    "                data[\"stride\"] = _scalarize(val)\n",
    "            elif key == \"kernel_size\":\n",
    "                data[\"kernel_size\"] = _scalarize(val)\n",
    "            elif key == \"pool_size\":\n",
    "                data[\"pool_size\"] = _scalarize(val)\n",
    "            else:\n",
    "                data[key] = val\n",
    "\n",
    "    act = cfg.get(\"activation\", None)\n",
    "    valid_acts = SUPPORTED_LAYERS.get(ltype, {}).get(\"valid_activations\", [])\n",
    "    if act and act not in valid_acts:\n",
    "        raise ValueError(f\"Unsupported activation '{act}' for layer {layer.name}\")\n",
    "    if act:\n",
    "        data[\"activation\"] = act\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Fixed-point / MIF helpers\n",
    "# ----------------------------\n",
    "def quantize_to_fixed_raw(value, fraction_bits, data_width, signed=True):\n",
    "    scaled = value * (2 ** fraction_bits)\n",
    "    q = int(round(scaled))\n",
    "    if signed:\n",
    "        max_val = 2 ** (data_width - 1) - 1\n",
    "        min_val = -(2 ** (data_width - 1))\n",
    "        q = max(min(q, max_val), min_val)\n",
    "        if q < 0:\n",
    "            q = (1 << data_width) + q\n",
    "    else:\n",
    "        max_val = 2 ** data_width - 1\n",
    "        q = max(min(q, max_val), 0)\n",
    "    return q\n",
    "\n",
    "\n",
    "def int_to_bin(value, data_width):\n",
    "    return format(value, f\"0{data_width}b\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Hardware JSON builder helpers (Conv2D / Max2D)\n",
    "# ----------------------------\n",
    "GLOBAL_DEFAULTS = {\n",
    "    \"DATA_WIDTH\": 16,\n",
    "    \"FRACTION_SIZE\": 14,\n",
    "    \"SIGNED\": 1,\n",
    "    \"GUARD_TYPE\": 2\n",
    "}\n",
    "\n",
    "\n",
    "def _make_conv2d_block(name, layer, conv_idx,\n",
    "                       prev_out_sig=\"data_in\", prev_valid_sig=\"data_valid\",\n",
    "                       data_width_sym=\"DATA_WIDTH\", frac_sym=\"FRACTION_SIZE\",\n",
    "                       signed_sym=\"SIGNED\", guard_sym=\"GUARD_TYPE\"):\n",
    "    \"\"\"\n",
    "    Create Conv2D block JSON (with previous layer output connections).\n",
    "    \"\"\"\n",
    "    activation_flag = 1 if layer.get(\"activation\", \"\").lower() == \"relu\" else 0\n",
    "\n",
    "    params = {\n",
    "        \"KERNEL_SIZE\": layer[\"kernel_size\"],\n",
    "        \"COLUMN_NUM\": layer[\"input_shape\"][0],\n",
    "        \"ROW_NUM\": layer[\"input_shape\"][1],\n",
    "        \"STRIDE\": layer[\"stride\"],\n",
    "        \"INPUT_CHANNELS\": layer[\"input_channels\"],\n",
    "        \"OUTPUT_CHANNELS\": layer.get(\"output_channels\", layer.get(\"filters\", 1)),\n",
    "        \"DATA_WIDTH\": data_width_sym,\n",
    "        \"FRACTION_SIZE\": frac_sym,\n",
    "        \"SIGNED\": signed_sym,\n",
    "        \"ACTIVATION\": activation_flag,\n",
    "        \"GUARD_TYPE\": guard_sym,\n",
    "        \"WEIGHT_FILE\": f\"c_{conv_idx}_w.mif\",\n",
    "        \"BIAS_FILE\": f\"c_{conv_idx}_b.mif\"\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        name: {\n",
    "            \"module\": \"Conv2D\",\n",
    "            \"parameters\": params,\n",
    "            \"inputs\": {\n",
    "                \"clock\": {\"name\": \"clock\", \"width\": 1},\n",
    "                \"sreset_n\": {\"name\": \"sreset_n\", \"width\": 1},\n",
    "                \"data_valid\": {\"name\": prev_valid_sig, \"width\": 1},\n",
    "                \"data_in\": {\"name\": prev_out_sig, \"width\": data_width_sym}\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"conv_out\": {\"name\": f\"{name}_out\", \"width\": data_width_sym},\n",
    "                \"conv_valid\": {\"name\": f\"{name}_valid\", \"width\": 1}\n",
    "            },\n",
    "            \"files\": {\n",
    "                \"weights_file\": params[\"WEIGHT_FILE\"],\n",
    "                \"biases_file\": params[\"BIAS_FILE\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _make_max2d_block(name, layer, pool_idx,\n",
    "                      prev_out_sig=\"data_in\", prev_valid_sig=\"data_valid\",\n",
    "                      data_width_sym=\"DATA_WIDTH\", signed_sym=\"SIGNED\"):\n",
    "    \"\"\"\n",
    "    Create Max2D block JSON (with previous layer output connections).\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"KERNEL_SIZE\": layer[\"pool_size\"],\n",
    "        \"DATA_WIDTH\": data_width_sym,\n",
    "        \"COLUMN_NUM\": layer[\"input_shape\"][0],\n",
    "        \"ROW_NUM\": layer[\"input_shape\"][1],\n",
    "        \"STRIDE\": layer[\"stride\"],\n",
    "        \"CHANNELS\": layer[\"input_channels\"],\n",
    "        \"SIGNED\": signed_sym\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        name: {\n",
    "            \"module\": \"Max2D\",\n",
    "            \"parameters\": params,\n",
    "            \"inputs\": {\n",
    "                \"clock\": {\"name\": \"clock\", \"width\": 1},\n",
    "                \"sreset_n\": {\"name\": \"sreset_n\", \"width\": 1},\n",
    "                \"data_valid\": {\"name\": prev_valid_sig, \"width\": 1},\n",
    "                \"data_in\": {\"name\": prev_out_sig, \"width\": data_width_sym}\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"maxp_out\": {\"name\": f\"{name}_out\", \"width\": data_width_sym},\n",
    "                \"maxp_valid\": {\"name\": f\"{name}_valid\", \"width\": 1}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _make_maxfinder_block(prev_out_signal, num_outputs):\n",
    "    return {\n",
    "        \"maxFinder\": {\n",
    "            \"module\": \"maxFinder\",\n",
    "            \"parameters\": {\n",
    "                \"NUM_INPUTS\": num_outputs,\n",
    "                \"DATA_WIDTH\": \"DATA_WIDTH\",\n",
    "                \"SIGNED\": \"SIGNED\"\n",
    "            },\n",
    "            \"inputs\": {\n",
    "                \"clock\": {\"name\": \"clock\", \"width\": 1},\n",
    "                \"sreset_n\": {\"name\": \"sreset_n\", \"width\": 1},\n",
    "                \"data_valid\": {\"name\": f\"{prev_out_signal}_valid\", \"width\": 1},\n",
    "                \"data_in\": {\"name\": prev_out_signal, \"width\": f\"DATA_WIDTH * {num_outputs}\"}\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"class_idx\": {\"name\": \"class_idx\", \"width\": \"log2(NUM_INPUTS)\"},\n",
    "                \"valid\": {\"name\": \"class_valid\", \"width\": 1}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def _build_nn_block(prev_out_signal, dense_layers, prev_channels):\n",
    "    \"\"\"\n",
    "    Build NN metadata. Adds:\n",
    "      - name\n",
    "      - num_inputs (flattened size)\n",
    "      - input_channels (from last conv/pool)\n",
    "      - num_neurons\n",
    "      - activation\n",
    "      - weight/bias filename patterns\n",
    "    \"\"\"\n",
    "    nn_layers = []\n",
    "    for i, layer in enumerate(dense_layers, start=1):\n",
    "        # For the first dense layer after flatten, inherit real channel count\n",
    "        if i == 1:\n",
    "            input_ch = prev_channels\n",
    "        else:\n",
    "            # All subsequent dense layers are fully connected â†’ single channel\n",
    "            input_ch = 1\n",
    "\n",
    "        entry = {\n",
    "            \"name\": f\"dense_L{i}\",\n",
    "            \"num_inputs\": layer[\"input_shape\"][0],\n",
    "            \"input_channels\": input_ch,\n",
    "            \"num_neurons\": layer[\"units\"],\n",
    "            \"activation\": layer.get(\"activation\", \"none\"),\n",
    "            \"weight_file_pattern\": f\"w_{i}_<neuron>.mif\",\n",
    "            \"bias_file_pattern\": f\"b_{i}_<neuron>.mif\"\n",
    "        }\n",
    "        nn_layers.append(entry)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"Build\": True,\n",
    "        \"globals\": {\n",
    "            \"DATA_WIDTH\": \"DATA_WIDTH\",\n",
    "            \"FRACTION_SIZE\": \"FRACTION_SIZE\",\n",
    "            \"SIGNED\": \"SIGNED\"\n",
    "        },\n",
    "        \"io\": {\n",
    "            \"input_signal\": prev_out_signal,\n",
    "            \"input_valid\": f\"{prev_out_signal}_valid\",\n",
    "            \"output_signal\": \"nn_out\",\n",
    "            \"output_valid\": \"nn_valid\"\n",
    "        },\n",
    "        \"layers\": nn_layers\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# ModelGen class\n",
    "# ----------------------------\n",
    "class ModelGen:\n",
    "    def __init__(self, model_path, out_dir=\"project_output\"):\n",
    "        \"\"\"\n",
    "        model_path : path to .keras model file\n",
    "        out_dir    : project folder where Memory_Files will be created\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.out_dir = Path(out_dir)\n",
    "        self.memory_root = self.out_dir / \"Memory_Files\"\n",
    "        self.software_json_path = self.out_dir / \"software_model.json\"\n",
    "        self.hardware_json_path = self.out_dir / \"hardware_model.json\"\n",
    "        self.hardware_params = GLOBAL_DEFAULTS.copy()\n",
    "        self.software_json = None\n",
    "        self.hardware_json = None\n",
    "\n",
    "    # ---------------- Stage 1 ----------------\n",
    "    def generate_software_json(self, out_path=None):\n",
    "        if out_path is None:\n",
    "            out_path = self.software_json_path\n",
    "        model = tf.keras.models.load_model(str(self.model_path), compile=False)\n",
    "        layer_shapes = _infer_layer_shapes(model)\n",
    "        layers_out = []\n",
    "        for layer, (in_s, out_s) in zip(model.layers, layer_shapes):\n",
    "            ltype = layer.__class__.__name__\n",
    "            if ltype not in SUPPORTED_LAYERS:\n",
    "                # keep backward compatibility: skip unknown layers\n",
    "                continue\n",
    "            info = _extract_layer_info(layer, in_s, out_s)\n",
    "            layers_out.append(info)\n",
    "        # Ensure Dense after Flatten inherits correct channel count\n",
    "        for i in range(1, len(layers_out)):\n",
    "            prev_layer = layers_out[i - 1]\n",
    "            curr_layer = layers_out[i]\n",
    "            if curr_layer[\"type\"] == \"Dense\" and prev_layer[\"type\"] == \"Flatten\":\n",
    "                curr_layer[\"input_channels\"] = prev_layer[\"input_channels\"]\n",
    "\n",
    "        out_json = {\"model_name\": model.name, \"layers\": layers_out}\n",
    "        Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        Path(out_path).write_text(json.dumps(out_json, indent=2))\n",
    "        self.software_json = out_json\n",
    "        self.software_json_path = Path(out_path)\n",
    "        print(f\"âœ… Software model JSON written to {out_path}\")\n",
    "\n",
    "    # ---------------- Stage 2 ----------------\n",
    "    def compile(self, data_width=16, fraction_bits=14, signed=1, guard_type=2, add_mif_prefix=False):\n",
    "        \"\"\"\n",
    "        Generate hardware_model.json with proper layer chaining.\n",
    "        \"\"\"\n",
    "        self.hardware_params = {\n",
    "            \"DATA_WIDTH\": data_width,\n",
    "            \"FRACTION_SIZE\": fraction_bits,\n",
    "            \"SIGNED\": signed,\n",
    "            \"GUARD_TYPE\": guard_type\n",
    "        }\n",
    "\n",
    "        if self.software_json is None:\n",
    "            if self.software_json_path.exists():\n",
    "                self.software_json = json.loads(self.software_json_path.read_text())\n",
    "            else:\n",
    "                raise RuntimeError(\"software_model.json not found â€” run generate_software_json first.\")\n",
    "\n",
    "        layers = self.software_json[\"layers\"]\n",
    "        conv_idx, pool_idx = 1, 1\n",
    "        dense_layers = []\n",
    "        prev_out, prev_valid = \"data_in\", \"data_valid\"\n",
    "        prev_channels = 1\n",
    "\n",
    "        top = {\n",
    "            \"module\": \"CNN\",\n",
    "            \"hardware parameters\": self.hardware_params,\n",
    "            \"inputs\": {\"clock\": 1, \"sreset_n\": 1, \"data_in\": \"DATA_WIDTH\", \"data_valid\": 1},\n",
    "            \"outputs\": {\"class_idx\": \"log2(NUM_INPUTS)\", \"class_valid\": 1}\n",
    "        }\n",
    "\n",
    "        for layer in layers:\n",
    "            ltype = layer[\"type\"]\n",
    "\n",
    "            if ltype == \"Conv2D\":\n",
    "                conv_name = f\"Conv{conv_idx}\"\n",
    "                block = _make_conv2d_block(conv_name, layer, conv_idx,\n",
    "                                        prev_out_sig=prev_out,\n",
    "                                        prev_valid_sig=prev_valid)\n",
    "                top.update(block)\n",
    "                prev_out, prev_valid = f\"{conv_name}_out\", f\"{conv_name}_valid\"\n",
    "                prev_channels = layer.get(\"output_channels\", prev_channels)\n",
    "                conv_idx += 1\n",
    "\n",
    "            elif ltype == \"MaxPooling2D\":\n",
    "                pool_name = f\"Maxpool{pool_idx}\"\n",
    "                block = _make_max2d_block(pool_name, layer, pool_idx,\n",
    "                                        prev_out_sig=prev_out,\n",
    "                                        prev_valid_sig=prev_valid)\n",
    "                top.update(block)\n",
    "                prev_out, prev_valid = f\"{pool_name}_out\", f\"{pool_name}_valid\"\n",
    "                prev_channels = layer.get(\"output_channels\", prev_channels)\n",
    "                pool_idx += 1\n",
    "\n",
    "            elif ltype == \"Flatten\":\n",
    "                continue\n",
    "\n",
    "            elif ltype == \"Dense\":\n",
    "                dense_layers.append(layer)\n",
    "\n",
    "        # NN + maxFinder hookup\n",
    "        if dense_layers:\n",
    "            top[\"NN\"] = _build_nn_block(prev_out, dense_layers, prev_channels)\n",
    "            last_layer = dense_layers[-1]\n",
    "            num_outputs = last_layer[\"units\"]\n",
    "            top.update(_make_maxfinder_block(\"nn_out\", num_outputs))\n",
    "        else:\n",
    "            top[\"outputs\"] = {\"data_out\": prev_out, \"data_valid\": prev_valid}\n",
    "\n",
    "        hw = {\"Hardware_model\": {\"top\": top}}\n",
    "        self.hardware_json = hw\n",
    "        self.hardware_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.hardware_json_path.write_text(json.dumps(hw, indent=2))\n",
    "        print(f\"âœ… Hardware model JSON written to {self.hardware_json_path}\")\n",
    "\n",
    "\n",
    "    # ---------------- Stage 3 ----------------\n",
    "    def gen_files(self, save_index=True):\n",
    "        \"\"\"\n",
    "        Generate MIF and CSV files for weights and biases:\n",
    "         - Conv layers: one weights MIF (c_<i>_w.mif) and one biases MIF (c_<i>_b.mif)\n",
    "         - Dense layers: each neuron gets w_<layer>_<neuron>.mif and b_<layer>_<neuron>.mif\n",
    "        Files written to: <out_dir>/Memory_Files/mif  and <out_dir>/Memory_Files/csv\n",
    "        \"\"\"\n",
    "        model = tf.keras.models.load_model(str(self.model_path), compile=False)\n",
    "\n",
    "        data_width = int(self.hardware_params[\"DATA_WIDTH\"])\n",
    "        fraction_bits = int(self.hardware_params[\"FRACTION_SIZE\"])\n",
    "        signed = bool(self.hardware_params[\"SIGNED\"])\n",
    "\n",
    "        mem_root = self.memory_root\n",
    "        if mem_root.exists():\n",
    "            shutil.rmtree(mem_root)\n",
    "            print(f\"ğŸ§¹ Removed existing folder: {mem_root}\")\n",
    "        (mem_root / \"mif\").mkdir(parents=True, exist_ok=True)\n",
    "        (mem_root / \"csv\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        generated = {\"conv_blocks\": [], \"nn_layers\": []}\n",
    "        conv_block = 1\n",
    "        dense_layer = 1\n",
    "\n",
    "        def qfix(v):\n",
    "            return quantize_to_fixed_raw(v, fraction_bits, data_width, signed)\n",
    "\n",
    "        def write_file(path_obj, arr, mode):\n",
    "            path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(path_obj, \"w\") as f:\n",
    "                if mode == \"mif\":\n",
    "                    for val in arr:\n",
    "                        f.write(int_to_bin(int(val), data_width) + \"\\n\")\n",
    "                else:\n",
    "                    for val in arr:\n",
    "                        f.write(f\"{val}\\n\")\n",
    "\n",
    "        for layer in model.layers:\n",
    "            ltype = layer.__class__.__name__\n",
    "\n",
    "            if ltype == \"Conv2D\":\n",
    "                w, b = layer.get_weights()\n",
    "                w, b = np.array(w), np.array(b)\n",
    "                k_h, k_w, in_c, out_c = w.shape\n",
    "                # ordering: out_channel major, then in_channel, then kh, kw (kernel pos fastest)\n",
    "                ordered = []\n",
    "                for oc in range(out_c):\n",
    "                    for ic in range(in_c):\n",
    "                        for kh in range(k_h):\n",
    "                            for kw in range(k_w):\n",
    "                                ordered.append(float(w[kh, kw, ic, oc]))\n",
    "                qw = [qfix(v) for v in ordered]\n",
    "                qb = [qfix(float(x)) for x in b]\n",
    "\n",
    "                w_mif = mem_root / \"mif\" / f\"c_{conv_block}_w.mif\"\n",
    "                b_mif = mem_root / \"mif\" / f\"c_{conv_block}_b.mif\"\n",
    "                w_csv = mem_root / \"csv\" / f\"c_{conv_block}_w.csv\"\n",
    "                b_csv = mem_root / \"csv\" / f\"c_{conv_block}_b.csv\"\n",
    "\n",
    "                write_file(w_mif, qw, \"mif\")\n",
    "                write_file(b_mif, qb, \"mif\")\n",
    "                write_file(w_csv, ordered, \"csv\")\n",
    "                write_file(b_csv, b.tolist(), \"csv\")\n",
    "\n",
    "                generated[\"conv_blocks\"].append({\n",
    "                    \"conv_block\": conv_block,\n",
    "                    \"weights_mif\": str(w_mif),\n",
    "                    \"biases_mif\": str(b_mif),\n",
    "                    \"weights_csv\": str(w_csv),\n",
    "                    \"biases_csv\": str(b_csv)\n",
    "                })\n",
    "                print(f\"âœ… Conv block {conv_block}: {len(qw)} weights, {len(qb)} biases\")\n",
    "                conv_block += 1\n",
    "\n",
    "            elif ltype == \"Dense\":\n",
    "                w, b = layer.get_weights()\n",
    "                w, b = np.array(w), np.array(b)\n",
    "                in_dim, out_dim = w.shape\n",
    "                neuron_entries = []\n",
    "                for neuron in range(out_dim):\n",
    "                    weights_list = [float(w[inp, neuron]) for inp in range(in_dim)]\n",
    "                    bias_val = float(b[neuron])\n",
    "                    qw = [qfix(v) for v in weights_list]\n",
    "                    qb = qfix(bias_val)\n",
    "\n",
    "                    w_mif = mem_root / \"mif\" / f\"w_{dense_layer}_{neuron}.mif\"\n",
    "                    b_mif = mem_root / \"mif\" / f\"b_{dense_layer}_{neuron}.mif\"\n",
    "                    w_csv = mem_root / \"csv\" / f\"w_{dense_layer}_{neuron}.csv\"\n",
    "                    b_csv = mem_root / \"csv\" / f\"b_{dense_layer}_{neuron}.csv\"\n",
    "\n",
    "                    write_file(w_mif, qw, \"mif\")\n",
    "                    write_file(b_mif, [qb], \"mif\")\n",
    "                    write_file(w_csv, weights_list, \"csv\")\n",
    "                    write_file(b_csv, [bias_val], \"csv\")\n",
    "\n",
    "                    neuron_entries.append({\n",
    "                        \"neuron\": neuron,\n",
    "                        \"weights_mif\": str(w_mif),\n",
    "                        \"bias_mif\": str(b_mif),\n",
    "                        \"weights_csv\": str(w_csv),\n",
    "                        \"bias_csv\": str(b_csv)\n",
    "                    })\n",
    "                generated[\"nn_layers\"].append({\"layer\": dense_layer, \"neurons\": neuron_entries})\n",
    "                print(f\"âœ… Dense layer {dense_layer}: {out_dim} neurons\")\n",
    "                dense_layer += 1\n",
    "\n",
    "            else:\n",
    "                # skip or ignore Flatten, etc.\n",
    "                continue\n",
    "\n",
    "        if save_index:\n",
    "            index_file = mem_root / \"Memory_Index.json\"\n",
    "            with open(index_file, \"w\") as f:\n",
    "                json.dump(generated, f, indent=2)\n",
    "            print(f\"ğŸ—‚ï¸  Saved index file: {index_file}\")\n",
    "\n",
    "        print(f\"ğŸ“ All memory files saved in: {mem_root.resolve()}\")\n",
    "\n",
    "    # ---------------- Stage 4 ----------------\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Stage 4: Full RTL builder integrated.\n",
    "        Generates:\n",
    "        - <out_dir>/rtl/Layer_1.v, Layer_2.v, ...\n",
    "        - <out_dir>/rtl/{PROJECT_NAME}_top.v  (human readable)\n",
    "        Copies dependencies from designFiles/ (prefer self.out_dir/designFiles then ./designFiles)\n",
    "        Packages <out_dir>/hw with all .v (from rtl/) and .mif (from Memory_Files/mif/)\n",
    "        \"\"\"\n",
    "        from pathlib import Path\n",
    "        import shutil, json, os\n",
    "\n",
    "        OUT = Path(self.out_dir)\n",
    "        PROJECT_NAME = OUT.name\n",
    "        RTL_DIR = OUT / \"rtl\"\n",
    "        HW_DIR = OUT / \"hw\"\n",
    "\n",
    "        # Find designFiles directory (prefer project-specific then repo root)\n",
    "        DESIGN_DIR = OUT / \"designFiles\"\n",
    "        if not DESIGN_DIR.exists():\n",
    "            DESIGN_DIR = Path(\"designFiles\")\n",
    "        vprint = print\n",
    "\n",
    "        # dependencies base (same as earlier)\n",
    "        DEPENDENCY_BASE = {\n",
    "            \"Conv2D\": [\"FP_Adder.v\", \"FP_Multiplier.v\", \"ConvMemory.v\", \"Conv_SIC.v\", \"Conv_MIC.v\",\"imageBuffer.v\",\"imageBufferChnl.v\",\"pixelBuffer.v\",\"lineBuffer.v\"],\n",
    "            \"Max2D\": [\"FP_Comparator.v\", \"Maxpool.v\",\"imageBuffer.v\",\"imageBufferChnl.v\",\"pixelBuffer.v\",\"lineBuffer.v\"],\n",
    "            \"neuron\": [\"FP_Adder.v\", \"FP_Multiplier.v\", \"Weight_Memory.v\", \"relu.v\"],\n",
    "            \"maxFinder\": []\n",
    "        }\n",
    "\n",
    "        def copy_if_exists(src_path: Path, dst_dir: Path):\n",
    "            if not src_path.exists():\n",
    "                vprint(f\"âš  design file missing: {src_path}\")\n",
    "                return False\n",
    "            dst = dst_dir / src_path.name\n",
    "            if dst.exists() and dst.stat().st_size == src_path.stat().st_size:\n",
    "                return True\n",
    "            shutil.copy2(src_path, dst)\n",
    "            vprint(f\"â†’ Copied {src_path.name} -> {dst_dir}\")\n",
    "            return True\n",
    "\n",
    "        def resolve_and_copy(mod, rtl_dir=RTL_DIR):\n",
    "            # try several candidate filenames\n",
    "            candidates = [f\"{mod}.v\"]\n",
    "            if mod.lower() in (\"conv2d\",\"convch\",\"convolchnl\",\"convchnl\"):\n",
    "                candidates += [\"Conv2D.v\",\"ConvolChnl.v\",\"ConvChnl.v\"]\n",
    "            found = False\n",
    "            for c in candidates:\n",
    "                src = DESIGN_DIR / c\n",
    "                if src.exists():\n",
    "                    copy_if_exists(src, rtl_dir)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                vprint(f\"âš  Could not locate module source for {mod} (tried {candidates})\")\n",
    "            # copy dependency base\n",
    "            for dep in DEPENDENCY_BASE.get(mod, []):\n",
    "                copy_if_exists(DESIGN_DIR / dep, rtl_dir)\n",
    "\n",
    "        def ensure_keys(d, keys, ctx):\n",
    "            for k in keys:\n",
    "                if k not in d:\n",
    "                    raise KeyError(f\"[{ctx}] Missing mandatory parameter: '{k}'\")\n",
    "\n",
    "        def derive_prev_output_channels(top):\n",
    "            # find block immediately before NN and try to derive output channels\n",
    "            keys = list(top.keys())\n",
    "            try:\n",
    "                idx = keys.index(\"NN\")\n",
    "            except ValueError:\n",
    "                raise KeyError(\"NN block missing in JSON\")\n",
    "            prev_block = None\n",
    "            for k in reversed(keys[:idx]):\n",
    "                if isinstance(top[k], dict) and \"module\" in top[k]:\n",
    "                    prev_block = top[k]\n",
    "                    break\n",
    "            if prev_block is None:\n",
    "                raise KeyError(\"Could not find block before NN to derive output channels\")\n",
    "            params = prev_block.get(\"parameters\", {})\n",
    "            if \"OUTPUT_CHANNELS\" in params:\n",
    "                return int(params[\"OUTPUT_CHANNELS\"])\n",
    "            if \"OUTPUT_CHANNEL\" in params:\n",
    "                return int(params[\"OUTPUT_CHANNEL\"])\n",
    "            if \"CHANNELS\" in params:\n",
    "                return int(params[\"CHANNELS\"])\n",
    "            outs = prev_block.get(\"outputs\", {})\n",
    "            data_width = int(top[\"hardware parameters\"][\"DATA_WIDTH\"])\n",
    "            for oname, od in outs.items():\n",
    "                w = od.get(\"width\", None)\n",
    "                if isinstance(w, int):\n",
    "                    if w == data_width:\n",
    "                        return 1\n",
    "                    if w % data_width == 0:\n",
    "                        return w // data_width\n",
    "                if isinstance(w, str) and \"*\" in w:\n",
    "                    parts = [p.strip() for p in w.split(\"*\")]\n",
    "                    try:\n",
    "                        if parts[-1].isdigit():\n",
    "                            return int(parts[-1])\n",
    "                    except:\n",
    "                        pass\n",
    "            raise KeyError(\"Could not derive output channels from preceding block. Add OUTPUT_CHANNELS/CHANNELS to block parameters.\")\n",
    "\n",
    "        # Layer file generator (writes Layer_<i>.v)\n",
    "        def gen_layer_file(idx, layer, hw_params, first_layer_input_channels=None):\n",
    "            name = f\"Layer_{idx}\"\n",
    "            DW = int(hw_params[\"DATA_WIDTH\"])\n",
    "            FRAC = int(hw_params[\"FRACTION_SIZE\"])\n",
    "            weightIntWidth = DW - FRAC\n",
    "            SIGMOID_SIZE = hw_params.get(\"SIGMOID_SIZE\", 10)\n",
    "\n",
    "            # determine input channels\n",
    "            if \"input_channels\" in layer:\n",
    "                in_ch = int(layer[\"input_channels\"])\n",
    "            else:\n",
    "                if idx == 1:\n",
    "                    if first_layer_input_channels is None:\n",
    "                        raise KeyError(f\"Layer_{idx} missing 'input_channels' and couldn't derive it\")\n",
    "                    in_ch = int(first_layer_input_channels)\n",
    "                else:\n",
    "                    vprint(f\"âš  Layer_{idx} missing input_channels; defaulting to 1\")\n",
    "                    in_ch = 1\n",
    "\n",
    "            NN = int(layer[\"num_neurons\"])\n",
    "            numW = int(layer[\"num_inputs\"])\n",
    "            act = layer.get(\"activation\", \"relu\")\n",
    "            wpat = layer.get(\"weight_file_pattern\", f\"w_{idx}_<neuron>.mif\")\n",
    "            bpat = layer.get(\"bias_file_pattern\", f\"b_{idx}_<neuron>.mif\")\n",
    "\n",
    "            lines = []\n",
    "            lines.append(\"// =============================================================\")\n",
    "            lines.append(f\"// {name} â€” {layer.get('name','layer')}, act={act}, neurons={NN}\")\n",
    "            lines.append(\"// =============================================================\")\n",
    "            lines.append(f\"module {name} #(\")\n",
    "            lines.append(f\"    parameter NN = {NN},\")\n",
    "            lines.append(f\"    parameter numWeight = {numW},\")\n",
    "            lines.append(f\"    parameter dataWidth = {DW},\")\n",
    "            lines.append(f\"    parameter layerNum = {idx},\")\n",
    "            lines.append(f\"    parameter sigmoidSize = {SIGMOID_SIZE},\")\n",
    "            lines.append(f\"    parameter weightIntWidth = {weightIntWidth},\")\n",
    "            lines.append(f\"    parameter input_channels = {in_ch},\")\n",
    "            lines.append(f\"    parameter actType = \\\"{act}\\\"\")\n",
    "            lines.append(f\")(\")\n",
    "            lines.append(f\"    input           clk,\")\n",
    "            lines.append(f\"    input           rst,\")\n",
    "            lines.append(f\"    input           weightValid,\")\n",
    "            lines.append(f\"    input           biasValid,\")\n",
    "            lines.append(f\"    input  [31:0]   weightValue,\")\n",
    "            lines.append(f\"    input  [31:0]   biasValue,\")\n",
    "            lines.append(f\"    input  [31:0]   config_layer_num,\")\n",
    "            lines.append(f\"    input  [31:0]   config_neuron_num,\")\n",
    "            lines.append(f\"    input           x_valid,\")\n",
    "            lines.append(f\"    input  [input_channels*dataWidth-1:0] x_in,\")\n",
    "            lines.append(f\"    output [NN-1:0] o_valid,\")\n",
    "            lines.append(f\"    output [NN*dataWidth-1:0] x_out\")\n",
    "            lines.append(f\");\")\n",
    "            lines.append(\"\")\n",
    "\n",
    "            for n in range(NN):\n",
    "                wfile = wpat.replace(\"<neuron>\", str(n))\n",
    "                bfile = bpat.replace(\"<neuron>\", str(n))\n",
    "                lines.append(f\"    neuron #(\")\n",
    "                lines.append(f\"        .input_channels(input_channels),\")\n",
    "                lines.append(f\"        .numWeight(numWeight),\")\n",
    "                lines.append(f\"        .layerNo({idx}),\")\n",
    "                lines.append(f\"        .neuronNo({n}),\")\n",
    "                lines.append(f\"        .dataWidth(dataWidth),\")\n",
    "                lines.append(f\"        .sigmoidSize(sigmoidSize),\")\n",
    "                lines.append(f\"        .weightIntWidth(weightIntWidth),\")\n",
    "                lines.append(f\"        .actType(actType),\")\n",
    "                lines.append(f\"        .weightFile(\\\"{wfile}\\\"),\")\n",
    "                lines.append(f\"        .biasFile(\\\"{bfile}\\\")\")\n",
    "                lines.append(f\"    ) n_{n} (\")\n",
    "                lines.append(f\"        .clk(clk),\")\n",
    "                lines.append(f\"        .rst(rst),\")\n",
    "                lines.append(f\"        .myinput(x_in),\")\n",
    "                lines.append(f\"        .weightValid(weightValid),\")\n",
    "                lines.append(f\"        .biasValid(biasValid),\")\n",
    "                lines.append(f\"        .weightValue(weightValue),\")\n",
    "                lines.append(f\"        .biasValue(biasValue),\")\n",
    "                lines.append(f\"        .config_layer_num(config_layer_num),\")\n",
    "                lines.append(f\"        .config_neuron_num(config_neuron_num),\")\n",
    "                lines.append(f\"        .myinputValid(x_valid),\")\n",
    "                lines.append(f\"        .out(x_out[{n}*dataWidth +: dataWidth]),\")\n",
    "                lines.append(f\"        .outvalid(o_valid[{n}])\")\n",
    "                lines.append(f\"    );\")\n",
    "                lines.append(\"\")\n",
    "\n",
    "            lines.append(\"endmodule\")\n",
    "            (RTL_DIR / f\"{name}.v\").write_text(\"\\n\".join(lines))\n",
    "            vprint(f\"âœ“ Wrote {name}.v\")\n",
    "\n",
    "        # --------------------------------------\n",
    "        # Read hardware JSON and start building\n",
    "        # --------------------------------------\n",
    "        if RTL_DIR.exists():\n",
    "            shutil.rmtree(RTL_DIR)\n",
    "        RTL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        vprint(f\"Building RTL in: {RTL_DIR}\")\n",
    "\n",
    "        if not self.hardware_json_path.exists():\n",
    "            raise FileNotFoundError(f\"{self.hardware_json_path} not found â€” run compile() first\")\n",
    "\n",
    "        top = json.loads(self.hardware_json_path.read_text())[\"Hardware_model\"][\"top\"]\n",
    "\n",
    "        # copy some common dependencies (neuron, maxFinder, Conv2D/Max2D sources if exist)\n",
    "        for m in [\"Conv2D\", \"Max2D\", \"neuron\", \"maxFinder\"]:\n",
    "            resolve_and_copy(m, RTL_DIR)\n",
    "\n",
    "        # ---------- instantiate blocks up to NN ----------\n",
    "        keys = list(top.keys())\n",
    "        prev_block_name = None\n",
    "        prev_block = None\n",
    "        # We'll collect signal names for NN input\n",
    "        for k in keys:\n",
    "            if k in (\"hardware parameters\",\"inputs\",\"outputs\",\"module\",\"NN\",\"maxFinder\"):\n",
    "                continue\n",
    "            blk = top[k]\n",
    "            if not isinstance(blk, dict) or \"module\" not in blk:\n",
    "                continue\n",
    "            mod = blk[\"module\"]\n",
    "            params = blk.get(\"parameters\", {})\n",
    "            inputs = blk.get(\"inputs\", {})\n",
    "            outputs = blk.get(\"outputs\", {})\n",
    "\n",
    "            # Conv2D\n",
    "            if mod.lower() == \"conv2d\":\n",
    "                required = [\"KERNEL_SIZE\",\"COLUMN_NUM\",\"ROW_NUM\",\"STRIDE\",\"INPUT_CHANNELS\",\"OUTPUT_CHANNELS\",\"DATA_WIDTH\",\"FRACTION_SIZE\",\"SIGNED\",\"ACTIVATION\",\"GUARD_TYPE\",\"WEIGHT_FILE\",\"BIAS_FILE\"]\n",
    "                ensure_keys(params, required, f\"{k} Conv2D\")\n",
    "                out_ch = int(params[\"OUTPUT_CHANNELS\"])\n",
    "                conv_out_sig = outputs.get(\"conv_out\", {}).get(\"name\", f\"{k}_out\")\n",
    "                conv_valid_sig = outputs.get(\"conv_valid\", {}).get(\"name\", f\"{k}_valid\")\n",
    "\n",
    "                lines = [\n",
    "                    \"    // ------------------------------------------------------------\",\n",
    "                    f\"    // {k}: Conv2D\",\n",
    "                    \"    // ------------------------------------------------------------\",\n",
    "                    f\"    wire [{out_ch}*DATA_WIDTH-1:0] {conv_out_sig};\",\n",
    "                    f\"    wire {conv_valid_sig};\",\n",
    "                    \"\"\n",
    "                ]\n",
    "                # params\n",
    "                for pname, pval in params.items():\n",
    "                    if pname in (\"WEIGHT_FILE\",\"BIAS_FILE\"):\n",
    "                        lines.append(f\"    // param .{pname} => \\\"{pval}\\\"\")\n",
    "                    else:\n",
    "                        lines.append(f\"    // param .{pname} => {pval}\")\n",
    "                # instance\n",
    "                lines += [\n",
    "                    f\"    Conv2D #(\",\n",
    "                    # actual param list (pass literal for symbolic params)\n",
    "                ]\n",
    "                # formatted param list\n",
    "                for pname, pval in params.items():\n",
    "                    if pname in (\"WEIGHT_FILE\",\"BIAS_FILE\"):\n",
    "                        lines.append(f\"        .{pname}(\\\"{pval}\\\"),\")\n",
    "                    else:\n",
    "                        lines.append(f\"        .{pname}({pval}),\")\n",
    "                lines[-1] = lines[-1].rstrip(',')\n",
    "                lines += [\n",
    "                    f\"    ) {k} (\",\n",
    "                    f\"        .clock(clock),\",\n",
    "                    f\"        .sreset_n(sreset_n),\",\n",
    "                    f\"        .data_valid({inputs.get('data_valid',{}).get('name','data_valid')}),\",\n",
    "                    f\"        .data_in({inputs.get('data_in',{}).get('name','data_in')}),\",\n",
    "                    f\"        .conv_out({conv_out_sig}),\",\n",
    "                    f\"        .conv_valid({conv_valid_sig})\",\n",
    "                    f\"    );\",\n",
    "                    \"\"\n",
    "                ]\n",
    "                with open(RTL_DIR / f\"{k}.v\", \"w\") as f:\n",
    "                    f.write(\"\\n\".join(lines))\n",
    "                prev_block_name, prev_block = k, blk\n",
    "                vprint(f\"âœ“ Wrote {k}.v\")\n",
    "\n",
    "            # Max2D\n",
    "            elif mod.lower() == \"max2d\":\n",
    "                required = [\"KERNEL_SIZE\",\"DATA_WIDTH\",\"COLUMN_NUM\",\"ROW_NUM\",\"STRIDE\",\"CHANNELS\",\"SIGNED\"]\n",
    "                ensure_keys(params, required, f\"{k} Max2D\")\n",
    "                out_ch = int(params[\"CHANNELS\"])\n",
    "                max_out_sig = outputs.get(\"maxp_out\", {}).get(\"name\", f\"{k}_out\")\n",
    "                max_valid_sig = outputs.get(\"maxp_valid\", {}).get(\"name\", f\"{k}_valid\")\n",
    "\n",
    "                lines = [\n",
    "                    \"    // ------------------------------------------------------------\",\n",
    "                    f\"    // {k}: Max2D\",\n",
    "                    \"    // ------------------------------------------------------------\",\n",
    "                    f\"    wire [{out_ch}*DATA_WIDTH-1:0] {max_out_sig};\",\n",
    "                    f\"    wire {max_valid_sig};\",\n",
    "                    \"\"\n",
    "                ]\n",
    "                for pname, pval in params.items():\n",
    "                    lines.append(f\"    // param .{pname} => {pval}\")\n",
    "                lines += [\n",
    "                    f\"    Max2D #(\"\n",
    "                ]\n",
    "                for pname, pval in params.items():\n",
    "                    lines.append(f\"        .{pname}({pval}),\")\n",
    "                lines[-1] = lines[-1].rstrip(',')\n",
    "                lines += [\n",
    "                    f\"    ) {k} (\",\n",
    "                    f\"        .clock(clock),\",\n",
    "                    f\"        .sreset_n(sreset_n),\",\n",
    "                    f\"        .data_valid({inputs.get('data_valid',{}).get('name','data_valid')}),\",\n",
    "                    f\"        .data_in({inputs.get('data_in',{}).get('name','data_in')}),\",\n",
    "                    f\"        .maxp_out({max_out_sig}),\",\n",
    "                    f\"        .maxp_valid({max_valid_sig})\",\n",
    "                    f\"    );\",\n",
    "                    \"\"\n",
    "                ]\n",
    "                with open(RTL_DIR / f\"{k}.v\", \"w\") as f:\n",
    "                    f.write(\"\\n\".join(lines))\n",
    "                prev_block_name, prev_block = k, blk\n",
    "                vprint(f\"âœ“ Wrote {k}.v\")\n",
    "\n",
    "            else:\n",
    "                # Generic shallow instantiation (declare outputs and instantiate)\n",
    "                lines = [\n",
    "                    \"    // ------------------------------------------------------------\",\n",
    "                    f\"    // {k}: {mod} (generic)\",\n",
    "                    \"    // ------------------------------------------------------------\",\n",
    "                ]\n",
    "                # declare outputs\n",
    "                for oname, od in outputs.items():\n",
    "                    signame = od.get(\"name\", oname)\n",
    "                    w = od.get(\"width\", \"DATA_WIDTH\")\n",
    "                    lines.append(f\"    // output {signame} width={w}\")\n",
    "                lines += [\n",
    "                    f\"    {mod} #(\"\n",
    "                ]\n",
    "                for pname, pval in params.items():\n",
    "                    if isinstance(pval, str) and (pval.endswith(\".mif\") or pval.endswith(\".bin\")):\n",
    "                        lines.append(f\"        .{pname}(\\\"{pval}\\\"),\")\n",
    "                    else:\n",
    "                        lines.append(f\"        .{pname}({pval}),\")\n",
    "                if params:\n",
    "                    lines[-1] = lines[-1].rstrip(',')\n",
    "                lines += [\n",
    "                    f\"    ) {k} (\",\n",
    "                    f\"        .clock(clock),\",\n",
    "                    f\"        .sreset_n(sreset_n),\"\n",
    "                ]\n",
    "                for iname, idesc in inputs.items():\n",
    "                    conn = idesc.get(\"name\", iname)\n",
    "                    lines.append(f\"        .{iname}({conn}),\")\n",
    "                for oname, od in outputs.items():\n",
    "                    conn = od.get(\"name\", oname)\n",
    "                    lines.append(f\"        .{oname}({conn}),\")\n",
    "                # remove trailing comma\n",
    "                lines[-1] = lines[-1].rstrip(',')\n",
    "                lines += [\n",
    "                    f\"    );\",\n",
    "                    \"\"\n",
    "                ]\n",
    "                with open(RTL_DIR / f\"{k}.v\", \"w\") as f:\n",
    "                    f.write(\"\\n\".join(lines))\n",
    "                prev_block_name, prev_block = k, blk\n",
    "                vprint(f\"âœ“ Wrote {k}.v\")\n",
    "\n",
    "        # At this point prev_block is the block immediately before NN\n",
    "        if prev_block is None:\n",
    "            raise KeyError(\"No block found before NN; cannot connect NN input\")\n",
    "\n",
    "        # Determine first_in_ch\n",
    "        nn_block = top.get(\"NN\", {})\n",
    "        if not nn_block.get(\"Build\", False):\n",
    "            raise KeyError(\"NN Build flag not set or NN block missing\")\n",
    "        layers = nn_block.get(\"layers\", [])\n",
    "        if not layers:\n",
    "            raise KeyError(\"No layers listed in NN block\")\n",
    "\n",
    "        if \"input_channels\" in layers[0]:\n",
    "            first_in_ch = int(layers[0][\"input_channels\"])\n",
    "        else:\n",
    "            first_in_ch = derive_prev_output_channels(top)\n",
    "            vprint(f\"Derived first layer input_channels = {first_in_ch} from previous block '{prev_block_name}'\")\n",
    "\n",
    "        # Generate Layer_X.v files (unrolled neurons)\n",
    "        for i, layer in enumerate(layers, start=1):\n",
    "            gen_layer_file(i, layer, top[\"hardware parameters\"], first_layer_input_channels=first_in_ch if i==1 else None)\n",
    "            resolve_and_copy(\"neuron\", RTL_DIR)\n",
    "\n",
    "        # Generate {PROJECT_NAME}_top.v (full top with FSMs and NN chaining)\n",
    "        # We'll create a readable top file using the same logic as earlier v5\n",
    "        hwp = top.get(\"hardware parameters\", {})\n",
    "        ensure_keys(hwp, [\"DATA_WIDTH\",\"FRACTION_SIZE\",\"SIGNED\",\"GUARD_TYPE\"], \"hardware parameters\")\n",
    "        DW = int(hwp[\"DATA_WIDTH\"])\n",
    "        FRAC = int(hwp[\"FRACTION_SIZE\"])\n",
    "\n",
    "        tlines = []\n",
    "        tlines.append(\"// =============================================================\")\n",
    "        tlines.append(f\"// Auto-generated {PROJECT_NAME}_top (readable, JSON-driven)\")\n",
    "        tlines.append(\"// =============================================================\")\n",
    "        tlines.append(f\"module {PROJECT_NAME}_top #(\")\n",
    "        tlines.append(f\"    parameter DATA_WIDTH = {DW},\")\n",
    "        tlines.append(f\"    parameter FRACTION_SIZE = {hwp['FRACTION_SIZE']},\")\n",
    "        tlines.append(f\"    parameter SIGNED = {hwp['SIGNED']},\")\n",
    "        tlines.append(f\"    parameter GUARD_TYPE = {hwp['GUARD_TYPE']}\")\n",
    "        tlines.append(f\")(\")\n",
    "        tlines.append(f\"    input wire clock,\")\n",
    "        tlines.append(f\"    input wire sreset_n,\")\n",
    "        tlines.append(f\"    input wire [DATA_WIDTH-1:0] data_in,\")\n",
    "        tlines.append(f\"    input wire data_valid,\")\n",
    "        tlines.append(f\"    output wire [3:0] class_idx,\")\n",
    "        tlines.append(f\"    output wire class_valid\")\n",
    "        tlines.append(\");\")\n",
    "        tlines.append(\"\")\n",
    "        tlines.append(\"    wire reset = ~sreset_n;\")\n",
    "        tlines.append(\"\")\n",
    "\n",
    "        # Rebuild block wiring in the top file in order (up to NN)\n",
    "        prev_block_name2 = None\n",
    "        prev_block2 = None\n",
    "        for k in keys:\n",
    "            if k in (\"hardware parameters\",\"inputs\",\"outputs\",\"module\",\"NN\",\"maxFinder\"):\n",
    "                continue\n",
    "            blk = top[k]\n",
    "            if not isinstance(blk, dict) or \"module\" not in blk:\n",
    "                continue\n",
    "            params = blk.get(\"parameters\", {})\n",
    "            inputs = blk.get(\"inputs\", {})\n",
    "            outputs = blk.get(\"outputs\", {})\n",
    "            mod = blk[\"module\"]\n",
    "\n",
    "            if mod.lower() == \"conv2d\":\n",
    "                out_ch = int(params[\"OUTPUT_CHANNELS\"])\n",
    "                conv_out_sig = outputs.get(\"conv_out\", {}).get(\"name\", f\"{k}_out\")\n",
    "                conv_valid_sig = outputs.get(\"conv_valid\", {}).get(\"name\", f\"{k}_valid\")\n",
    "                tlines.append(\"    // ------------------------------------------------------------\")\n",
    "                tlines.append(f\"    // {k}: Conv2D\")\n",
    "                tlines.append(\"    // ------------------------------------------------------------\")\n",
    "                tlines.append(f\"    wire [{out_ch}*DATA_WIDTH-1:0] {conv_out_sig};\")\n",
    "                tlines.append(f\"    wire {conv_valid_sig};\")\n",
    "                tlines.append(f\"    Conv2D #(\")\n",
    "                for pname, pval in params.items():\n",
    "                    if pname in (\"WEIGHT_FILE\",\"BIAS_FILE\"):\n",
    "                        tlines.append(f\"        .{pname}(\\\"{pval}\\\"),\")\n",
    "                    else:\n",
    "                        tlines.append(f\"        .{pname}({pval}),\")\n",
    "                tlines[-1] = tlines[-1].rstrip(',')\n",
    "                tlines.append(f\"    ) {k} (\")\n",
    "                tlines.append(f\"        .clock(clock),\")\n",
    "                tlines.append(f\"        .sreset_n(sreset_n),\")\n",
    "                tlines.append(f\"        .data_valid({inputs.get('data_valid',{}).get('name','data_valid')}),\")\n",
    "                tlines.append(f\"        .data_in({inputs.get('data_in',{}).get('name','data_in')}),\")\n",
    "                tlines.append(f\"        .conv_out({conv_out_sig}),\")\n",
    "                tlines.append(f\"        .conv_valid({conv_valid_sig})\")\n",
    "                tlines.append(f\"    );\")\n",
    "                tlines.append(\"\")\n",
    "                prev_block_name2, prev_block2 = k, blk\n",
    "\n",
    "            elif mod.lower() == \"max2d\":\n",
    "                out_ch = int(params[\"CHANNELS\"])\n",
    "                max_out_sig = outputs.get(\"maxp_out\", {}).get(\"name\", f\"{k}_out\")\n",
    "                max_valid_sig = outputs.get(\"maxp_valid\", {}).get(\"name\", f\"{k}_valid\")\n",
    "                tlines.append(\"    // ------------------------------------------------------------\")\n",
    "                tlines.append(f\"    // {k}: Max2D\")\n",
    "                tlines.append(\"    // ------------------------------------------------------------\")\n",
    "                tlines.append(f\"    wire [{out_ch}*DATA_WIDTH-1:0] {max_out_sig};\")\n",
    "                tlines.append(f\"    wire {max_valid_sig};\")\n",
    "                tlines.append(f\"    Max2D #(\")\n",
    "                for pname, pval in params.items():\n",
    "                    tlines.append(f\"        .{pname}({pval}),\")\n",
    "                tlines[-1] = tlines[-1].rstrip(',')\n",
    "                tlines.append(f\"    ) {k} (\")\n",
    "                tlines.append(f\"        .clock(clock),\")\n",
    "                tlines.append(f\"        .sreset_n(sreset_n),\")\n",
    "                tlines.append(f\"        .data_valid({inputs.get('data_valid',{}).get('name','data_valid')}),\")\n",
    "                tlines.append(f\"        .data_in({inputs.get('data_in',{}).get('name','data_in')}),\")\n",
    "                tlines.append(f\"        .maxp_out({max_out_sig}),\")\n",
    "                tlines.append(f\"        .maxp_valid({max_valid_sig})\")\n",
    "                tlines.append(f\"    );\")\n",
    "                tlines.append(\"\")\n",
    "                prev_block_name2, prev_block2 = k, blk\n",
    "\n",
    "        # determine prev_data and prev_valid for NN\n",
    "        prev_outputs = prev_block.get(\"outputs\", {})\n",
    "        # pick first data-like signal and a valid signal\n",
    "        out_names = [od.get(\"name\", oname) for oname, od in prev_outputs.items()]\n",
    "        prev_data_sig = None\n",
    "        prev_valid_sig = None\n",
    "        for nm in out_names:\n",
    "            if nm and (\"conv_out\" in nm or \"maxp_out\" in nm or \"data_out\" in nm or \"kernel_out\" in nm):\n",
    "                prev_data_sig = nm\n",
    "                break\n",
    "        for nm in out_names:\n",
    "            if nm and (\"conv_valid\" in nm or \"maxp_valid\" in nm or \"data_valid\" in nm or \"valid\" in nm):\n",
    "                if nm != prev_data_sig:\n",
    "                    prev_valid_sig = nm\n",
    "                    break\n",
    "        if prev_data_sig is None:\n",
    "            # fallback to first output\n",
    "            prev_data_sig = out_names[0]\n",
    "        if prev_valid_sig is None:\n",
    "            prev_valid_sig = out_names[1] if len(out_names) > 1 else prev_data_sig + \"_valid\"\n",
    "\n",
    "        tlines.append(\"    // ------------------------------------------------------------\")\n",
    "        tlines.append(\"    // Neural Network Layers (with IDLE/SEND FSMs between layers)\")\n",
    "        tlines.append(\"    // ------------------------------------------------------------\")\n",
    "        tlines.append(\"    localparam IDLE = 1'b0;\")\n",
    "        tlines.append(\"    localparam SEND = 1'b1;\")\n",
    "        tlines.append(\"\")\n",
    "\n",
    "        prev_data = prev_data_sig\n",
    "        prev_valid = prev_valid_sig\n",
    "\n",
    "        prev_num_neurons = None\n",
    "        SIGMOID_SIZE = int(hwp.get(\"SIGMOID_SIZE\", 10))\n",
    "        weightIntWidth = DW - FRAC\n",
    "\n",
    "        for i, layer in enumerate(layers, start=1):\n",
    "            NN = int(layer[\"num_neurons\"])\n",
    "            numWeight = int(layer[\"num_inputs\"])\n",
    "            act = layer.get(\"activation\", \"relu\")\n",
    "            if \"input_channels\" in layer:\n",
    "                in_ch = int(layer[\"input_channels\"])\n",
    "            else:\n",
    "                if i == 1:\n",
    "                    in_ch = first_in_ch\n",
    "                else:\n",
    "                    in_ch = prev_num_neurons or 1\n",
    "            prev_num_neurons = NN\n",
    "\n",
    "            tlines.append(f\"    // ------------------------------------------------------------\")\n",
    "            tlines.append(f\"    // Layer {i} â€” {layer.get('name','layer')} ({act.upper()}, {NN} neurons)\")\n",
    "            tlines.append(f\"    // ------------------------------------------------------------\")\n",
    "            tlines.append(f\"    wire [{NN-1}:0] o{i}_valid;\")\n",
    "            tlines.append(f\"    wire [{NN}*DATA_WIDTH-1:0] x{i}_out;\")\n",
    "            tlines.append(f\"    reg  [{NN}*DATA_WIDTH-1:0] holdData_{i};\")\n",
    "            tlines.append(f\"    reg  [DATA_WIDTH-1:0] out_data_{i};\")\n",
    "            tlines.append(f\"    reg  data_out_valid_{i};\")\n",
    "            tlines.append(\"\")\n",
    "            tlines.append(f\"    Layer_{i} #(\")\n",
    "            tlines.append(f\"        .NN({NN}),\")\n",
    "            tlines.append(f\"        .numWeight({numWeight}),\")\n",
    "            tlines.append(f\"        .dataWidth(DATA_WIDTH),\")\n",
    "            tlines.append(f\"        .layerNum({i}),\")\n",
    "            tlines.append(f\"        .sigmoidSize({SIGMOID_SIZE}),\")\n",
    "            tlines.append(f\"        .weightIntWidth({weightIntWidth}),\")\n",
    "            tlines.append(f\"        .input_channels({in_ch}),\")\n",
    "            tlines.append(f\"        .actType(\\\"{act}\\\")\")\n",
    "            tlines.append(f\"    ) L{i} (\")\n",
    "            tlines.append(f\"        .clk(clock),\")\n",
    "            tlines.append(f\"        .rst(reset),\")\n",
    "            tlines.append(f\"        .weightValid(weightValid),\")\n",
    "            tlines.append(f\"        .biasValid(biasValid),\")\n",
    "            tlines.append(f\"        .weightValue(weightValue),\")\n",
    "            tlines.append(f\"        .biasValue(biasValue),\")\n",
    "            tlines.append(f\"        .config_layer_num(config_layer_num),\")\n",
    "            tlines.append(f\"        .config_neuron_num(config_neuron_num),\")\n",
    "            tlines.append(f\"        .x_valid({prev_valid}),\")\n",
    "            tlines.append(f\"        .x_in({prev_data}),\")\n",
    "            tlines.append(f\"        .o_valid(o{i}_valid),\")\n",
    "            tlines.append(f\"        .x_out(x{i}_out)\")\n",
    "            tlines.append(f\"    );\")\n",
    "            tlines.append(\"\")\n",
    "            # FSM\n",
    "            tlines.append(f\"    reg state_{i};\")\n",
    "            tlines.append(f\"    integer count_{i};\")\n",
    "            tlines.append(f\"    always @(posedge clock) begin\")\n",
    "            tlines.append(f\"        if (reset) begin\")\n",
    "            tlines.append(f\"            state_{i} <= IDLE;\")\n",
    "            tlines.append(f\"            count_{i} <= 0;\")\n",
    "            tlines.append(f\"            data_out_valid_{i} <= 0;\")\n",
    "            tlines.append(f\"        end else begin\")\n",
    "            tlines.append(f\"            case (state_{i})\")\n",
    "            tlines.append(f\"                IDLE: begin\")\n",
    "            tlines.append(f\"                    count_{i} <= 0;\")\n",
    "            tlines.append(f\"                    data_out_valid_{i} <= 0;\")\n",
    "            tlines.append(f\"                    if (o{i}_valid[0] == 1'b1) begin\")\n",
    "            tlines.append(f\"                        holdData_{i} <= x{i}_out;\")\n",
    "            tlines.append(f\"                        state_{i} <= SEND;\")\n",
    "            tlines.append(f\"                    end\")\n",
    "            tlines.append(f\"                end\")\n",
    "            tlines.append(f\"                SEND: begin\")\n",
    "            tlines.append(f\"                    out_data_{i} <= holdData_{i}[DATA_WIDTH-1:0];\")\n",
    "            tlines.append(f\"                    holdData_{i} <= holdData_{i} >> DATA_WIDTH;\")\n",
    "            tlines.append(f\"                    count_{i} <= count_{i} + 1;\")\n",
    "            tlines.append(f\"                    data_out_valid_{i} <= 1;\")\n",
    "            tlines.append(f\"                    if (count_{i} == {NN}) begin\")\n",
    "            tlines.append(f\"                        state_{i} <= IDLE;\")\n",
    "            tlines.append(f\"                        data_out_valid_{i} <= 0;\")\n",
    "            tlines.append(f\"                    end\")\n",
    "            tlines.append(f\"                end\")\n",
    "            tlines.append(f\"            endcase\")\n",
    "            tlines.append(f\"        end\")\n",
    "            tlines.append(f\"    end\")\n",
    "            tlines.append(\"\")\n",
    "\n",
    "            prev_data = f\"out_data_{i}\"\n",
    "            prev_valid = f\"data_out_valid_{i}\"\n",
    "\n",
    "        # final layer handling + maxFinder\n",
    "        last_layer = layers[-1]\n",
    "        last_act = last_layer.get(\"activation\",\"\").lower()\n",
    "        last_NN = int(last_layer[\"num_neurons\"])\n",
    "        if last_act in (\"softmax\",\"hardmax\"):\n",
    "            tlines.append(\"    // ------------------------------------------------------------\")\n",
    "            tlines.append(\"    // Final-layer packed hold + maxFinder\")\n",
    "            tlines.append(\"    // ------------------------------------------------------------\")\n",
    "            tlines.append(f\"    reg [{last_NN}*DATA_WIDTH-1:0] holdData_final;\")\n",
    "            tlines.append(f\"    always @(posedge clock) begin\")\n",
    "            tlines.append(f\"        if (o{len(layers)}_valid[0] == 1'b1)\")\n",
    "            tlines.append(f\"            holdData_final <= x{len(layers)}_out;\")\n",
    "            tlines.append(f\"    end\")\n",
    "            tlines.append(\"\")\n",
    "            if \"maxFinder\" in top:\n",
    "                mf = top[\"maxFinder\"]\n",
    "                mp = mf.get(\"parameters\", {})\n",
    "                ensure_keys(mp, [\"NUM_INPUTS\",\"DATA_WIDTH\",\"SIGNED\"], \"maxFinder parameters\")\n",
    "                tlines.append(f\"    maxFinder #(\")\n",
    "                tlines.append(f\"        .NUM_INPUTS({mp['NUM_INPUTS']}),\")\n",
    "                tlines.append(f\"        .DATA_WIDTH(DATA_WIDTH),\")\n",
    "                tlines.append(f\"        .SIGNED(SIGNED)\")\n",
    "                tlines.append(f\"    ) mFind (\")\n",
    "                tlines.append(f\"        .i_clk(clock),\")\n",
    "                tlines.append(f\"        .i_data(x{len(layers)}_out),\")\n",
    "                tlines.append(f\"        .i_valid(o{len(layers)}_valid),\")\n",
    "                tlines.append(f\"        .o_data(class_idx),\")\n",
    "                tlines.append(f\"        .o_data_valid(class_valid)\")\n",
    "                tlines.append(f\"    );\")\n",
    "                resolve_and_copy(\"maxFinder\", RTL_DIR)\n",
    "            else:\n",
    "                raise KeyError(\"maxFinder block missing in top JSON\")\n",
    "        else:\n",
    "            tlines.append(\"    // Final layer isn't softmax/hardmax â€” streamed outputs available at prev_data/prev_valid\")\n",
    "            tlines.append(\"\")\n",
    "\n",
    "        tlines.append(\"endmodule\")\n",
    "        (RTL_DIR / f\"{PROJECT_NAME}_top.v\").write_text(\"\\n\".join(tlines))\n",
    "        vprint(f\"âœ“ Wrote rtl/{PROJECT_NAME}_top.v\")\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # packaging: clean hw folder and copy files\n",
    "        # ---------------------------------------\n",
    "        if HW_DIR.exists():\n",
    "            shutil.rmtree(HW_DIR)\n",
    "        HW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # copy all .v into hw\n",
    "        for vfile in RTL_DIR.glob(\"*.v\"):\n",
    "            shutil.copy2(vfile, HW_DIR / vfile.name)\n",
    "        # copy dependency .v too\n",
    "        for dv in RTL_DIR.glob(\"*.v\"):\n",
    "            # already copied; kept for compatibility if dependencies were stored elsewhere\n",
    "            pass\n",
    "\n",
    "        # copy all .mif from Memory_Files/mif\n",
    "        mif_dir = self.memory_root / \"mif\"\n",
    "        if mif_dir.exists():\n",
    "            for m in mif_dir.glob(\"*.mif\"):\n",
    "                shutil.copy2(m, HW_DIR / m.name)\n",
    "\n",
    "        vprint(f\"ğŸ“¦ HW folder packaged: {HW_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf5009a",
   "metadata": {},
   "source": [
    "### Model Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fad3a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Software model JSON written to CNN3\\software_model.json\n"
     ]
    }
   ],
   "source": [
    "modelGen = ModelGen(\"model2.keras\",out_dir=\"CNN3\")\n",
    "modelGen.generate_software_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03788d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hardware model JSON written to CNN3\\hardware_model.json\n"
     ]
    }
   ],
   "source": [
    "modelGen.compile(data_width=16, fraction_bits=14, signed=1, guard_type=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38a91ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conv block 1: 54 weights, 2 biases\n",
      "âœ… Conv block 2: 36 weights, 2 biases\n",
      "âœ… Dense layer 1: 64 neurons\n",
      "âœ… Dense layer 2: 32 neurons\n",
      "âœ… Dense layer 3: 10 neurons\n",
      "ğŸ—‚ï¸  Saved index file: CNN3\\Memory_Files\\Memory_Index.json\n",
      "ğŸ“ All memory files saved in: E:\\Projects-\\Hardware-Design-Automation-for-CNNs\\CNN_Files\\ModelGen\\NewModelGen\\CNN3\\Memory_Files\n"
     ]
    }
   ],
   "source": [
    "modelGen.gen_files(save_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b79244d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RTL in: CNN3\\rtl\n",
      "â†’ Copied Conv2D.v -> CNN3\\rtl\n",
      "â†’ Copied FP_Adder.v -> CNN3\\rtl\n",
      "â†’ Copied FP_Multiplier.v -> CNN3\\rtl\n",
      "â†’ Copied ConvMemory.v -> CNN3\\rtl\n",
      "â†’ Copied Conv_SIC.v -> CNN3\\rtl\n",
      "â†’ Copied Conv_MIC.v -> CNN3\\rtl\n",
      "â†’ Copied relu.v -> CNN3\\rtl\n",
      "â†’ Copied Max2D.v -> CNN3\\rtl\n",
      "â†’ Copied FP_Comparator.v -> CNN3\\rtl\n",
      "â†’ Copied Maxpool.v -> CNN3\\rtl\n",
      "â†’ Copied neuron.v -> CNN3\\rtl\n",
      "â†’ Copied Weight_Memory.v -> CNN3\\rtl\n",
      "â†’ Copied maxFinder.v -> CNN3\\rtl\n",
      "âœ“ Wrote Conv1.v\n",
      "âœ“ Wrote Conv2.v\n",
      "âœ“ Wrote Maxpool1.v\n",
      "âœ“ Wrote Layer_1.v\n",
      "âœ“ Wrote Layer_2.v\n",
      "âœ“ Wrote Layer_3.v\n",
      "âœ“ Wrote rtl/{PROJECT_NAME}_top.v\n",
      "ğŸ“¦ HW folder packaged: E:\\Projects-\\Hardware-Design-Automation-for-CNNs\\CNN_Files\\ModelGen\\NewModelGen\\CNN3\\hw\n"
     ]
    }
   ],
   "source": [
    "modelGen.build()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
