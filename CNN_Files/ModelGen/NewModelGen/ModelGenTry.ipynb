{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876c418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606c0bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Apps_and_Programs\\anaconda\\envs\\CNN_TF_GPU\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">169</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,880</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m1\u001b[0m)      â”‚            \u001b[38;5;34m10\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m169\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m10,880\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚           \u001b[38;5;34m330\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,300</span> (51.95 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,300\u001b[0m (51.95 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,300</span> (51.95 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,300\u001b[0m (51.95 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(1,(3, 3),strides=(1,1), activation='relu', padding='valid',input_shape=(28,28,1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.save(\"model1.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272db0c9",
   "metadata": {},
   "source": [
    "### Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a89b491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading Keras model: model1.keras\n",
      "âš™ï¸ Inferring layer shapes...\n",
      "ğŸ§© Extracting layer configurations...\n",
      "âœ… Software model JSON written to software_model.json\n"
     ]
    }
   ],
   "source": [
    "# stage1_sw_gen.py\n",
    "\"\"\"\n",
    "Stage 1: TensorFlow/Keras model â†’ software_model.json\n",
    "---------------------------------\n",
    "Generates a hardware-friendly JSON description of the network.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ---------------- CONFIG ---------------- #\n",
    "SUPPORTED_LAYERS = {\n",
    "    \"Conv2D\": {\n",
    "        \"supported\": [\"filters\", \"kernel_size\", \"strides\", \"activation\"],\n",
    "        \"valid_activations\": [\"relu\", None],\n",
    "    },\n",
    "    \"MaxPooling2D\": {\n",
    "        \"supported\": [\"pool_size\", \"strides\"]\n",
    "    },\n",
    "    \"Dense\": {\n",
    "        \"supported\": [\"units\", \"activation\"],\n",
    "        \"valid_activations\": [\"relu\", \"sigmoid\", \"softmax\"],\n",
    "    },\n",
    "    \"Flatten\": {\"supported\": []},\n",
    "}\n",
    "\n",
    "# ---------------- UTILITY FUNCTIONS ---------------- #\n",
    "\n",
    "def _shape_to_list(shape):\n",
    "    \"\"\"Converts TensorShape/tuple to a clean Python list.\"\"\"\n",
    "    if shape is None:\n",
    "        return None\n",
    "    return [int(s) if s is not None else None for s in shape]\n",
    "\n",
    "def _scalarize(param):\n",
    "    \"\"\"Convert tuple/list params like (3,3) -> 3 (requires square kernels).\"\"\"\n",
    "    if isinstance(param, (list, tuple)):\n",
    "        if len(set(param)) == 1:\n",
    "            return param[0]\n",
    "        raise ValueError(f\"âš ï¸ Only square kernels supported, got {param}\")\n",
    "    return param\n",
    "\n",
    "def _infer_layer_shapes(model):\n",
    "    \"\"\"Run dummy inference to infer input/output tensor shapes per layer.\"\"\"\n",
    "    if hasattr(model, \"inputs\") and model.inputs:\n",
    "        input_shape = [1 if s is None else s for s in model.inputs[0].shape]\n",
    "    elif hasattr(model, \"input_shape\") and model.input_shape:\n",
    "        input_shape = [1 if s is None else s for s in model.input_shape]\n",
    "    else:\n",
    "        raise ValueError(\"âŒ Could not infer model input shape.\")\n",
    "\n",
    "    dummy_input = tf.zeros(input_shape)\n",
    "    x = dummy_input\n",
    "    layer_shapes = []\n",
    "    for layer in model.layers:\n",
    "        in_shape = _shape_to_list(x.shape)\n",
    "        x = layer(x)\n",
    "        out_shape = _shape_to_list(x.shape)\n",
    "        layer_shapes.append((in_shape, out_shape))\n",
    "    return layer_shapes\n",
    "\n",
    "def _extract_layer_info(layer, in_shape, out_shape):\n",
    "    \"\"\"Extracts simplified hardware-useful info per layer.\"\"\"\n",
    "    ltype = layer.__class__.__name__\n",
    "    cfg = layer.get_config()\n",
    "    data = {\"name\": layer.name, \"type\": ltype}\n",
    "\n",
    "    # ---------------- Shapes ---------------- #\n",
    "    if len(in_shape) == 4:\n",
    "        _, in_h, in_w, in_c = in_shape\n",
    "        data[\"input_shape\"] = [in_h, in_w]\n",
    "        data[\"input_channels\"] = in_c\n",
    "    elif len(in_shape) == 2:\n",
    "        _, n = in_shape\n",
    "        data[\"input_shape\"] = [n]\n",
    "        data[\"input_channels\"] = 1\n",
    "\n",
    "    if len(out_shape) == 4:\n",
    "        _, out_h, out_w, out_c = out_shape\n",
    "        data[\"output_shape\"] = [out_h, out_w]\n",
    "        data[\"output_channels\"] = out_c\n",
    "    elif len(out_shape) == 2:\n",
    "        _, n = out_shape\n",
    "        data[\"output_shape\"] = [n]\n",
    "        data[\"output_channels\"] = n\n",
    "\n",
    "    # ---------------- Parameters ---------------- #\n",
    "    for key in SUPPORTED_LAYERS.get(ltype, {}).get(\"supported\", []):\n",
    "        if key in cfg:\n",
    "            val = cfg[key]\n",
    "            if key == \"strides\":\n",
    "                data[\"stride\"] = _scalarize(val)\n",
    "            elif key == \"kernel_size\":\n",
    "                data[\"kernel_size\"] = _scalarize(val)\n",
    "            elif key == \"pool_size\":\n",
    "                data[\"pool_size\"] = _scalarize(val)\n",
    "            else:\n",
    "                data[key] = val\n",
    "\n",
    "    # ---------------- Activation ---------------- #\n",
    "    act = cfg.get(\"activation\", None)\n",
    "    valid_acts = SUPPORTED_LAYERS.get(ltype, {}).get(\"valid_activations\", [])\n",
    "    if act and act not in valid_acts:\n",
    "        raise ValueError(f\"Unsupported activation '{act}' for layer {layer.name}\")\n",
    "    if act:\n",
    "        data[\"activation\"] = act\n",
    "\n",
    "    return data\n",
    "\n",
    "# ---------------- MAIN FUNCTION ---------------- #\n",
    "\n",
    "def generate_software_json(model_path, out_path=\"software_model.json\"):\n",
    "    \"\"\"Generates the software_model.json from a Keras model file.\"\"\"\n",
    "    print(f\"ğŸ“¥ Loading Keras model: {model_path}\")\n",
    "    model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "    print(f\"âš™ï¸ Inferring layer shapes...\")\n",
    "    layer_shapes = _infer_layer_shapes(model)\n",
    "\n",
    "    print(f\"ğŸ§© Extracting layer configurations...\")\n",
    "    layers_out = []\n",
    "    for layer, (in_s, out_s) in zip(model.layers, layer_shapes):\n",
    "        ltype = layer.__class__.__name__\n",
    "        if ltype not in SUPPORTED_LAYERS:\n",
    "            print(f\"âš ï¸ Skipping unsupported layer type: {ltype}\")\n",
    "            continue\n",
    "        info = _extract_layer_info(layer, in_s, out_s)\n",
    "        layers_out.append(info)\n",
    "\n",
    "    out_json = {\n",
    "        \"model_name\": model.name,\n",
    "        \"layers\": layers_out\n",
    "    }\n",
    "\n",
    "    Path(out_path).write_text(json.dumps(out_json, indent=2))\n",
    "    print(f\"âœ… Software model JSON written to {out_path}\")\n",
    "    return out_json\n",
    "\n",
    "# ---------------- Example Run ---------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    generate_software_json(\"model1.keras\", \"software_model.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c808b2",
   "metadata": {},
   "source": [
    "### Stage 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4604a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Reading software_model.json ...\n",
      "âœ… Hardware model written to hardware_model.json\n"
     ]
    }
   ],
   "source": [
    "# stage2_hw_gen.py\n",
    "\"\"\"\n",
    "Stage 2: software_model.json â†’ hardware_model.json\n",
    "--------------------------------------------------\n",
    "Generates hardware-level JSON from software description JSON.\n",
    "\n",
    "âœ“ Separate counters for Conv / Pool / NN layers\n",
    "âœ“ Conv MIF names match 'Memory_Files/mif' convention\n",
    "âœ“ Neural network section includes full layer information\n",
    "âœ“ NN file structure consistent with per-neuron MIF generation\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# --------------------------- CONFIG --------------------------- #\n",
    "\n",
    "GLOBAL_DEFAULTS = {\n",
    "    \"DATA_WIDTH\": 16,\n",
    "    \"FRACTION_SIZE\": 14,\n",
    "    \"SIGNED\": 1,\n",
    "    \"GUARD_TYPE\": 2\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------- BLOCK BUILDERS --------------------------- #\n",
    "\n",
    "def make_imgbuf_block(name, layer, prev_out, stride, channels, kernel_size):\n",
    "    \"\"\"Create ImageBufferChnl block JSON.\"\"\"\n",
    "    return {\n",
    "        name: {\n",
    "            \"module\": \"ImageBufferChnl\",\n",
    "            \"parameters\": {\n",
    "                \"KERNEL_SIZE\": kernel_size,\n",
    "                \"DATA_WIDTH\": \"DATA_WIDTH\",\n",
    "                \"COLUMN_NUM\": layer[\"input_shape\"][0],\n",
    "                \"ROW_NUM\": layer[\"input_shape\"][1],\n",
    "                \"STRIDE\": stride,\n",
    "                \"CHANNELS\": channels\n",
    "            },\n",
    "            \"inputs\": {\n",
    "                \"clock\": {\"name\": \"clock\", \"width\": 1},\n",
    "                \"sreset_n\": {\"name\": \"sreset_n\", \"width\": 1},\n",
    "                \"data_valid\": {\"name\": \"data_valid\" if prev_out == \"data_in\" else f\"{prev_out}_valid\", \"width\": 1},\n",
    "                \"data_in\": {\"name\": prev_out, \"width\": \"DATA_WIDTH\"}\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"data_out\": {\"name\": f\"{name}_out\", \"width\": \"DATA_WIDTH\"},\n",
    "                \"kernel_out\": {\"name\": f\"kernel_out_{name}\", \"width\": f\"DATA_WIDTH * {kernel_size} * {kernel_size}\"},\n",
    "                \"kernel_valid\": {\"name\": f\"kernel_valid_{name}\", \"width\": 1}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def make_conv_block(name, layer, ibuf_name, conv_block_num):\n",
    "    \"\"\"Create ConvChnl block JSON with correct MIF filenames.\"\"\"\n",
    "    k = layer[\"kernel_size\"]\n",
    "    return {\n",
    "        name: {\n",
    "            \"module\": \"ConvChnl\",\n",
    "            \"parameters\": {\n",
    "                \"KERNEL_SIZE\": k,\n",
    "                \"CHANNELS\": layer[\"input_channels\"],\n",
    "                \"DATA_WIDTH\": \"DATA_WIDTH\",\n",
    "                \"FRACTION_SIZE\": \"FRACTION_SIZE\",\n",
    "                \"SIGNED\": \"SIGNED\",\n",
    "                \"ACTIVATION\": layer.get(\"activation\", \"none\"),\n",
    "                \"GUARD_TYPE\": \"GUARD_TYPE\"\n",
    "            },\n",
    "            \"files\": {\n",
    "                \"weights_file\": f\"c_{conv_block_num}_w.mif\",\n",
    "                \"biases_file\": f\"c_{conv_block_num}_b.mif\"\n",
    "            },\n",
    "            \"inputs\": {\n",
    "                \"clock\": {\"name\": \"clock\", \"width\": 1},\n",
    "                \"sreset_n\": {\"name\": \"sreset_n\", \"width\": 1},\n",
    "                \"data_valid\": {\"name\": f\"kernel_valid_{ibuf_name}\", \"width\": 1},\n",
    "                \"kernel_in\": {\"name\": f\"kernel_out_{ibuf_name}\", \"width\": f\"DATA_WIDTH * {k} * {k}\"}\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"conv_out\": {\"name\": f\"{name}_out\", \"width\": \"DATA_WIDTH\"},\n",
    "                \"conv_valid\": {\"name\": f\"{name}_valid\", \"width\": 1}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def make_maxpool_block(name, layer, ibuf_name):\n",
    "    \"\"\"Create MaxpoolChnl block JSON.\"\"\"\n",
    "    k = layer[\"pool_size\"]\n",
    "    return {\n",
    "        name: {\n",
    "            \"module\": \"MaxpoolChnl\",\n",
    "            \"parameters\": {\n",
    "                \"KERNEL_SIZE\": k,\n",
    "                \"DATA_WIDTH\": \"DATA_WIDTH\",\n",
    "                \"SIGNED\": \"SIGNED\",\n",
    "                \"CHANNELS\": layer[\"input_channels\"]\n",
    "            },\n",
    "            \"inputs\": {\n",
    "                \"clock\": {\"name\": \"clock\", \"width\": 1},\n",
    "                \"sreset_n\": {\"name\": \"sreset_n\", \"width\": 1},\n",
    "                \"data_valid\": {\"name\": f\"kernel_valid_{ibuf_name}\", \"width\": 1},\n",
    "                \"kernel_in\": {\"name\": f\"kernel_out_{ibuf_name}\", \"width\": f\"DATA_WIDTH * {k} * {k}\"}\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"maxp_out\": {\"name\": f\"{name}_out\", \"width\": \"DATA_WIDTH\"},\n",
    "                \"maxp_valid\": {\"name\": f\"{name}_valid\", \"width\": 1}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def make_maxfinder_block(prev_out_signal, num_outputs):\n",
    "    \"\"\"Create maxFinder block as final classifier stage.\"\"\"\n",
    "    return {\n",
    "        \"maxFinder\": {\n",
    "            \"module\": \"maxFinder\",\n",
    "            \"parameters\": {\n",
    "                \"NUM_INPUTS\": num_outputs,\n",
    "                \"DATA_WIDTH\": \"DATA_WIDTH\",\n",
    "                \"SIGNED\": \"SIGNED\"\n",
    "            },\n",
    "            \"inputs\": {\n",
    "                \"clock\": {\"name\": \"clock\", \"width\": 1},\n",
    "                \"sreset_n\": {\"name\": \"sreset_n\", \"width\": 1},\n",
    "                \"data_valid\": {\"name\": f\"{prev_out_signal}_valid\", \"width\": 1},\n",
    "                \"data_in\": {\"name\": prev_out_signal, \"width\": f\"DATA_WIDTH * {num_outputs}\"}\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"class_idx\": {\"name\": \"class_idx\", \"width\": \"log2(NUM_INPUTS)\"},\n",
    "                \"valid\": {\"name\": \"class_valid\", \"width\": 1}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def build_nn_block(prev_out_signal, dense_layers):\n",
    "    \"\"\"\n",
    "    Build NN structure describing Dense layers.\n",
    "    File naming remains consistent with neuron-per-MIF generation.\n",
    "    \"\"\"\n",
    "    nn_layers = []\n",
    "    for i, layer in enumerate(dense_layers, start=1):\n",
    "        entry = {\n",
    "            \"name\": f\"dense_L{i}\",\n",
    "            \"num_inputs\": layer[\"input_shape\"][0],\n",
    "            \"num_neurons\": layer[\"units\"],\n",
    "            \"activation\": layer.get(\"activation\", \"none\"),\n",
    "            # NOTE: MIF file naming convention placeholders\n",
    "            \"weight_file_pattern\": f\"w_{i}_<neuron>.mif\",\n",
    "            \"bias_file_pattern\": f\"b_{i}_<neuron>.mif\"\n",
    "        }\n",
    "        nn_layers.append(entry)\n",
    "\n",
    "    return {\n",
    "        \"Build\": True,\n",
    "        \"globals\": {\n",
    "            \"DATA_WIDTH\": \"DATA_WIDTH\",\n",
    "            \"FRACTION_SIZE\": \"FRACTION_SIZE\",\n",
    "            \"SIGNED\": \"SIGNED\"\n",
    "        },\n",
    "        \"io\": {\n",
    "            \"input_signal\": prev_out_signal,\n",
    "            \"input_valid\": f\"{prev_out_signal}_valid\",\n",
    "            \"output_signal\": \"nn_out\",\n",
    "            \"output_valid\": \"nn_valid\"\n",
    "        },\n",
    "        \"layers\": nn_layers\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------- MAIN CONVERSION --------------------------- #\n",
    "\n",
    "def generate_hardware_json(input_file=\"software_model.json\", output_file=\"hardware_model.json\"):\n",
    "    \"\"\"Main Stage 2 entry point.\"\"\"\n",
    "    print(f\"ğŸ“„ Reading {input_file} ...\")\n",
    "    with open(input_file, \"r\") as f:\n",
    "        model = json.load(f)\n",
    "\n",
    "    layers = model[\"layers\"]\n",
    "    globals_ = GLOBAL_DEFAULTS.copy()\n",
    "\n",
    "    top = {\n",
    "        \"module\": \"CNN\",\n",
    "        \"hardware parameters\": globals_,\n",
    "        \"inputs\": {\n",
    "            \"clock\": 1,\n",
    "            \"sreset_n\": 1,\n",
    "            \"data_in\": \"DATA_WIDTH\",\n",
    "            \"data_valid\": 1\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"class_idx\": \"log2(NUM_INPUTS)\",\n",
    "            \"class_valid\": 1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Independent counters\n",
    "    conv_idx = 1\n",
    "    pool_idx = 1\n",
    "    dense_layers = []\n",
    "    prev_out = \"data_in\"\n",
    "\n",
    "    for layer in layers:\n",
    "        ltype = layer[\"type\"]\n",
    "\n",
    "        if ltype == \"Conv2D\":\n",
    "            ibuf_name = f\"ImageBuffer_Conv{conv_idx}\"\n",
    "            conv_name = f\"Conv{conv_idx}\"\n",
    "\n",
    "            top.update(make_imgbuf_block(\n",
    "                ibuf_name, layer, prev_out,\n",
    "                stride=layer[\"stride\"],\n",
    "                channels=layer[\"input_channels\"],\n",
    "                kernel_size=layer[\"kernel_size\"]\n",
    "            ))\n",
    "            top.update(make_conv_block(conv_name, layer, ibuf_name, conv_idx))\n",
    "\n",
    "            prev_out = f\"{conv_name}_out\"\n",
    "            conv_idx += 1\n",
    "\n",
    "        elif ltype == \"MaxPooling2D\":\n",
    "            ibuf_name = f\"ImageBuffer_Maxpool{pool_idx}\"\n",
    "            pool_name = f\"Maxpool{pool_idx}\"\n",
    "\n",
    "            top.update(make_imgbuf_block(\n",
    "                ibuf_name, layer, prev_out,\n",
    "                stride=layer[\"stride\"],\n",
    "                channels=layer[\"input_channels\"],\n",
    "                kernel_size=layer[\"pool_size\"]\n",
    "            ))\n",
    "            top.update(make_maxpool_block(pool_name, layer, ibuf_name))\n",
    "\n",
    "            prev_out = f\"{pool_name}_out\"\n",
    "            pool_idx += 1\n",
    "\n",
    "        elif ltype == \"Flatten\":\n",
    "            continue\n",
    "\n",
    "        elif ltype == \"Dense\":\n",
    "            dense_layers.append(layer)\n",
    "\n",
    "    # Add NN and MaxFinder\n",
    "    if dense_layers:\n",
    "        top[\"NN\"] = build_nn_block(prev_out, dense_layers)\n",
    "        last_layer = dense_layers[-1]\n",
    "        num_outputs = last_layer[\"units\"]\n",
    "        top.update(make_maxfinder_block(\"nn_out\", num_outputs))\n",
    "    else:\n",
    "        top[\"outputs\"] = {\n",
    "            \"data_out\": prev_out,\n",
    "            \"data_valid\": f\"{prev_out}_valid\"\n",
    "        }\n",
    "\n",
    "    final_json = {\"Hardware_model\": {\"top\": top}}\n",
    "\n",
    "    Path(output_file).write_text(json.dumps(final_json, indent=2))\n",
    "    print(f\"âœ… Hardware model written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_hardware_json(\"software_model.json\", \"hardware_model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff112a",
   "metadata": {},
   "source": [
    "### MIF Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23f3929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Removed existing folder: Memory_Files\n",
      "âœ… Conv block 1: 9 weights, 1 biases\n",
      "âœ… Neuron 0 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 1 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 2 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 3 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 4 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 5 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 6 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 7 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 8 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 9 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 10 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 11 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 12 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 13 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 14 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 15 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 16 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 17 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 18 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 19 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 20 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 21 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 22 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 23 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 24 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 25 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 26 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 27 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 28 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 29 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 30 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 31 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 32 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 33 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 34 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 35 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 36 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 37 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 38 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 39 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 40 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 41 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 42 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 43 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 44 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 45 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 46 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 47 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 48 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 49 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 50 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 51 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 52 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 53 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 54 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 55 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 56 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 57 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 58 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 59 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 60 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 61 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 62 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 63 in layer 1: 169 weights, 1 bias\n",
      "âœ… Neuron 0 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 1 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 2 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 3 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 4 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 5 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 6 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 7 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 8 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 9 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 10 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 11 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 12 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 13 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 14 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 15 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 16 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 17 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 18 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 19 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 20 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 21 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 22 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 23 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 24 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 25 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 26 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 27 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 28 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 29 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 30 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 31 in layer 2: 64 weights, 1 bias\n",
      "âœ… Neuron 0 in layer 3: 32 weights, 1 bias\n",
      "âœ… Neuron 1 in layer 3: 32 weights, 1 bias\n",
      "âœ… Neuron 2 in layer 3: 32 weights, 1 bias\n",
      "âœ… Neuron 3 in layer 3: 32 weights, 1 bias\n",
      "âœ… Neuron 4 in layer 3: 32 weights, 1 bias\n",
      "âœ… Neuron 5 in layer 3: 32 weights, 1 bias\n",
      "âœ… Neuron 6 in layer 3: 32 weights, 1 bias\n",
      "âœ… Neuron 7 in layer 3: 32 weights, 1 bias\n",
      "âœ… Neuron 8 in layer 3: 32 weights, 1 bias\n",
      "âœ… Neuron 9 in layer 3: 32 weights, 1 bias\n",
      "ğŸ—‚ï¸  Saved index file: Memory_Files\\Memory_Index.json\n",
      "\n",
      "ğŸ“ All files generated under: E:\\Projects-\\Hardware-Design-Automation-for-CNNs\\CNN_Files\\ModelGen\\NewModelGen\\Memory_Files\n"
     ]
    }
   ],
   "source": [
    "# mif_writer.py\n",
    "\"\"\"\n",
    "MIF Writer Utility (Final Stable Version)\n",
    "-----------------------------------------\n",
    "Generates .mif (binary) and .csv (float) files for Conv2D and Dense (neuron)\n",
    "layers, and optionally saves a Memory_Index.json describing all files.\n",
    "\n",
    "Features:\n",
    "---------\n",
    "â€¢ Writes binary data in .mif (ready for $readmemb).\n",
    "â€¢ Cleans previous output folder automatically.\n",
    "â€¢ Uses default output folder 'Memory_Files' if none provided.\n",
    "â€¢ Optionally saves a JSON index file when save_index=True.\n",
    "â€¢ Returns nothing (pure side-effect operation).\n",
    "\"\"\"\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ------------------ Fixed-Point Conversion ------------------ #\n",
    "def quantize_to_fixed(value, fraction_bits, data_width, signed=True):\n",
    "    \"\"\"Convert float â†’ fixed-point integer (twoâ€™s complement).\"\"\"\n",
    "    scaled = value * (2 ** fraction_bits)\n",
    "    q = int(round(scaled))\n",
    "\n",
    "    if signed:\n",
    "        max_val = 2 ** (data_width - 1) - 1\n",
    "        min_val = -(2 ** (data_width - 1))\n",
    "        q = max(min(q, max_val), min_val)\n",
    "        if q < 0:\n",
    "            q = (1 << data_width) + q  # two's complement\n",
    "    else:\n",
    "        max_val = 2 ** data_width - 1\n",
    "        q = max(min(q, max_val), 0)\n",
    "\n",
    "    return q\n",
    "\n",
    "\n",
    "def int_to_bin(value, data_width):\n",
    "    \"\"\"Convert integer to zero-padded binary string.\"\"\"\n",
    "    return format(value, f\"0{data_width}b\")\n",
    "\n",
    "\n",
    "def write_list(filepath, values, data_width, fmt=\"mif\"):\n",
    "    \"\"\"Write values to file (.mif for binary, .csv for floats).\"\"\"\n",
    "    Path(filepath).parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        if fmt == \"mif\":\n",
    "            for val in values:\n",
    "                f.write(int_to_bin(val, data_width) + \"\\n\")\n",
    "        elif fmt == \"csv\":\n",
    "            for val in values:\n",
    "                f.write(f\"{val}\\n\")\n",
    "\n",
    "\n",
    "# ------------------ Conv2D Export ------------------ #\n",
    "def generate_conv_mif_files(weights, biases, conv_block_num, data_width, fraction_bits, signed, base_dir):\n",
    "    \"\"\"Generate .mif/.csv files for a Conv2D layer.\"\"\"\n",
    "    w = np.array(weights)\n",
    "    b = np.array(biases)\n",
    "    k_h, k_w, in_ch, out_ch = w.shape\n",
    "    ordered_values = []\n",
    "\n",
    "    # Order: OC major â†’ IC â†’ kernel (kernel position fastest)\n",
    "    for oc in range(out_ch):\n",
    "        for ic in range(in_ch):\n",
    "            for kh in range(k_h):\n",
    "                for kw in range(k_w):\n",
    "                    ordered_values.append(float(w[kh, kw, ic, oc]))\n",
    "\n",
    "    q_weights = [quantize_to_fixed(v, fraction_bits, data_width, signed) for v in ordered_values]\n",
    "    q_biases = [quantize_to_fixed(float(x), fraction_bits, data_width, signed) for x in b]\n",
    "\n",
    "    mif_dir = Path(base_dir) / \"mif\"\n",
    "    csv_dir = Path(base_dir) / \"csv\"\n",
    "\n",
    "    # File paths\n",
    "    w_mif = mif_dir / f\"c_{conv_block_num}_w.mif\"\n",
    "    b_mif = mif_dir / f\"c_{conv_block_num}_b.mif\"\n",
    "    w_csv = csv_dir / f\"c_{conv_block_num}_w.csv\"\n",
    "    b_csv = csv_dir / f\"c_{conv_block_num}_b.csv\"\n",
    "\n",
    "    write_list(w_mif, q_weights, data_width, fmt=\"mif\")\n",
    "    write_list(b_mif, q_biases, data_width, fmt=\"mif\")\n",
    "    write_list(w_csv, ordered_values, data_width, fmt=\"csv\")\n",
    "    write_list(b_csv, b.tolist(), data_width, fmt=\"csv\")\n",
    "\n",
    "    print(f\"âœ… Conv block {conv_block_num}: {len(q_weights)} weights, {len(q_biases)} biases\")\n",
    "\n",
    "    return {\n",
    "        \"conv_block\": conv_block_num,\n",
    "        \"weights_mif\": str(w_mif),\n",
    "        \"biases_mif\": str(b_mif),\n",
    "        \"weights_csv\": str(w_csv),\n",
    "        \"biases_csv\": str(b_csv)\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------ Dense Export (Per Neuron) ------------------ #\n",
    "def generate_dense_mif_files(weights, biases, layer_number, data_width, fraction_bits, signed, base_dir):\n",
    "    \"\"\"Generate per-neuron .mif/.csv files for one Dense layer.\"\"\"\n",
    "    w = np.array(weights)\n",
    "    b = np.array(biases)\n",
    "    in_dim, out_dim = w.shape\n",
    "\n",
    "    mif_dir = Path(base_dir) / \"mif\"\n",
    "    csv_dir = Path(base_dir) / \"csv\"\n",
    "\n",
    "    neuron_files = []\n",
    "    for neuron in range(out_dim):\n",
    "        weights_list = [float(w[inp, neuron]) for inp in range(in_dim)]\n",
    "        bias_val = float(b[neuron])\n",
    "\n",
    "        q_weights = [quantize_to_fixed(v, fraction_bits, data_width, signed) for v in weights_list]\n",
    "        q_bias = quantize_to_fixed(bias_val, fraction_bits, data_width, signed)\n",
    "\n",
    "        # File paths\n",
    "        w_mif = mif_dir / f\"w_{layer_number}_{neuron}.mif\"\n",
    "        b_mif = mif_dir / f\"b_{layer_number}_{neuron}.mif\"\n",
    "        w_csv = csv_dir / f\"w_{layer_number}_{neuron}.csv\"\n",
    "        b_csv = csv_dir / f\"b_{layer_number}_{neuron}.csv\"\n",
    "\n",
    "        write_list(w_mif, q_weights, data_width, fmt=\"mif\")\n",
    "        write_list(b_mif, [q_bias], data_width, fmt=\"mif\")\n",
    "        write_list(w_csv, weights_list, data_width, fmt=\"csv\")\n",
    "        write_list(b_csv, [bias_val], data_width, fmt=\"csv\")\n",
    "\n",
    "        neuron_files.append({\n",
    "            \"neuron\": neuron,\n",
    "            \"weights_mif\": str(w_mif),\n",
    "            \"bias_mif\": str(b_mif),\n",
    "            \"weights_csv\": str(w_csv),\n",
    "            \"bias_csv\": str(b_csv)\n",
    "        })\n",
    "\n",
    "        print(f\"âœ… Neuron {neuron} in layer {layer_number}: \"\n",
    "              f\"{len(weights_list)} weights, 1 bias\")\n",
    "\n",
    "    return {\"layer\": layer_number, \"neurons\": neuron_files}\n",
    "\n",
    "\n",
    "# ------------------ Master Export ------------------ #\n",
    "def export_mifs_from_keras(model_path,\n",
    "                           data_width=16,\n",
    "                           fraction_bits=14,\n",
    "                           signed=True,\n",
    "                           base_dir=None,\n",
    "                           save_index=True):\n",
    "    \"\"\"\n",
    "    Generates .mif (binary) and .csv (float) files for Conv2D and Dense layers.\n",
    "    Cleans output folder each run. Optionally saves Memory_Index.json.\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "    # Default folder\n",
    "    if base_dir is None:\n",
    "        base_dir = \"Memory_Files\"\n",
    "\n",
    "    base_dir = Path(base_dir)\n",
    "\n",
    "    # Clean existing folder\n",
    "    if base_dir.exists():\n",
    "        shutil.rmtree(base_dir)\n",
    "        print(f\"ğŸ§¹ Removed existing folder: {base_dir}\")\n",
    "\n",
    "    (base_dir / \"mif\").mkdir(parents=True, exist_ok=True)\n",
    "    (base_dir / \"csv\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    conv_block_num = 1\n",
    "    nn_layer_num = 1\n",
    "    generated = {\"conv_blocks\": [], \"nn_layers\": []}\n",
    "\n",
    "    # Generate MIFs\n",
    "    for layer in model.layers:\n",
    "        ltype = layer.__class__.__name__\n",
    "\n",
    "        if ltype == \"Conv2D\":\n",
    "            w, b = layer.get_weights()\n",
    "            files = generate_conv_mif_files(w, b, conv_block_num, data_width, fraction_bits, signed, base_dir)\n",
    "            generated[\"conv_blocks\"].append(files)\n",
    "            conv_block_num += 1\n",
    "\n",
    "        elif ltype == \"Dense\":\n",
    "            w, b = layer.get_weights()\n",
    "            files = generate_dense_mif_files(w, b, nn_layer_num, data_width, fraction_bits, signed, base_dir)\n",
    "            generated[\"nn_layers\"].append(files)\n",
    "            nn_layer_num += 1\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Optional JSON index\n",
    "    if save_index:\n",
    "        index_path = base_dir / \"Memory_Index.json\"\n",
    "        with open(index_path, \"w\") as f:\n",
    "            json.dump(generated, f, indent=2)\n",
    "        print(f\"ğŸ—‚ï¸  Saved index file: {index_path}\")\n",
    "\n",
    "    print(f\"\\nğŸ“ All files generated under: {base_dir.resolve()}\")\n",
    "    # No return value (intentional)\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_mifs_from_keras(\n",
    "    model_path=\"model.keras\",\n",
    "    data_width=16,\n",
    "    fraction_bits=14,\n",
    "    signed=True,\n",
    "    save_index=True   # set False to skip Memory_Index.json\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d0b123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Copied Conv2D.v -> rtl/\n",
      "â†’ Copied FP_Adder.v -> rtl/\n",
      "â†’ Copied FP_Multiplier.v -> rtl/\n",
      "â†’ Copied ConvMemory.v -> rtl/\n",
      "â†’ Copied Conv_SIC.v -> rtl/\n",
      "â†’ Copied Conv_MIC.v -> rtl/\n",
      "â†’ Copied relu.v -> rtl/\n",
      "â†’ Copied Max2D.v -> rtl/\n",
      "â†’ Copied FP_Comparator.v -> rtl/\n",
      "â†’ Copied Maxpool.v -> rtl/\n",
      "â†’ Copied neuron.v -> rtl/\n",
      "â†’ Copied FP_Adder.v -> rtl/\n",
      "â†’ Copied FP_Multiplier.v -> rtl/\n",
      "â†’ Copied Weight_Memory.v -> rtl/\n",
      "â†’ Copied Sig_ROM.v -> rtl/\n",
      "â†’ Copied relu.v -> rtl/\n",
      "â†’ Copied maxFinder.v -> rtl/\n",
      "â†’ Copied Conv2D.v -> rtl/\n",
      "â†’ Copied FP_Adder.v -> rtl/\n",
      "â†’ Copied FP_Multiplier.v -> rtl/\n",
      "â†’ Copied ConvMemory.v -> rtl/\n",
      "â†’ Copied Conv_SIC.v -> rtl/\n",
      "â†’ Copied Conv_MIC.v -> rtl/\n",
      "â†’ Copied relu.v -> rtl/\n",
      "â†’ Copied Max2D.v -> rtl/\n",
      "â†’ Copied FP_Comparator.v -> rtl/\n",
      "â†’ Copied Maxpool.v -> rtl/\n",
      "âœ“ Wrote Layer_1.v\n",
      "â†’ Copied neuron.v -> rtl/\n",
      "â†’ Copied FP_Adder.v -> rtl/\n",
      "â†’ Copied FP_Multiplier.v -> rtl/\n",
      "â†’ Copied Weight_Memory.v -> rtl/\n",
      "â†’ Copied Sig_ROM.v -> rtl/\n",
      "â†’ Copied relu.v -> rtl/\n",
      "âœ“ Wrote Layer_2.v\n",
      "â†’ Copied neuron.v -> rtl/\n",
      "â†’ Copied FP_Adder.v -> rtl/\n",
      "â†’ Copied FP_Multiplier.v -> rtl/\n",
      "â†’ Copied Weight_Memory.v -> rtl/\n",
      "â†’ Copied Sig_ROM.v -> rtl/\n",
      "â†’ Copied relu.v -> rtl/\n",
      "âœ“ Wrote Layer_3.v\n",
      "â†’ Copied neuron.v -> rtl/\n",
      "â†’ Copied FP_Adder.v -> rtl/\n",
      "â†’ Copied FP_Multiplier.v -> rtl/\n",
      "â†’ Copied Weight_Memory.v -> rtl/\n",
      "â†’ Copied Sig_ROM.v -> rtl/\n",
      "â†’ Copied relu.v -> rtl/\n",
      "â†’ Copied maxFinder.v -> rtl/\n",
      "âœ“ Wrote rtl/CNN_top.v\n",
      "\n",
      "âœ… build_rtl_v5 complete. Inspect rtl/*.v\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "build_rtl_v5.py\n",
    "\n",
    "Fully fixed & JSON-driven RTL builder:\n",
    " - copies designFiles/*.v -> rtl/\n",
    " - generates rtl/CNN_top.v (readable) and rtl/Layer_X.v files\n",
    " - order-agnostic: detects which block precedes NN\n",
    " - uses JSON-defined output names for Conv2D / Max2D (no _conv_out/_maxp_out suffixes)\n",
    " - generates FSMs between NN layers, and final holdData + maxFinder for softmax/hardmax\n",
    " - enforces mandatory parameters and strict validation\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ---------- Config ----------\n",
    "INPUT_JSON = \"hardware_model.json\"\n",
    "DESIGN_DIR = Path(\"designFiles\")\n",
    "RTL_DIR = Path(\"rtl\")\n",
    "VERBOSE = True\n",
    "\n",
    "DEPENDENCY_BASE = {\n",
    "    \"Conv2D\": [\"FP_Adder.v\", \"FP_Multiplier.v\", \"ConvMemory.v\", \"Conv_SIC.v\", \"Conv_MIC.v\", \"relu.v\"],\n",
    "    \"Max2D\": [\"FP_Comparator.v\", \"Maxpool.v\"],\n",
    "    \"neuron\": [\"FP_Adder.v\", \"FP_Multiplier.v\", \"Weight_Memory.v\", \"Sig_ROM.v\", \"relu.v\"],\n",
    "    \"maxFinder\": []\n",
    "}\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def vprint(*a):\n",
    "    if VERBOSE:\n",
    "        print(*a)\n",
    "\n",
    "def ensure_dirs():\n",
    "    RTL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_json():\n",
    "    p = Path(INPUT_JSON)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{INPUT_JSON} not found in cwd={os.getcwd()}\")\n",
    "    with open(p, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    if \"Hardware_model\" not in d or \"top\" not in d[\"Hardware_model\"]:\n",
    "        raise KeyError(\"hardware_model.json must have Hardware_model.top\")\n",
    "    return d[\"Hardware_model\"][\"top\"]\n",
    "\n",
    "def copy_if_exists(fname):\n",
    "    src = DESIGN_DIR / fname\n",
    "    dst = RTL_DIR / fname\n",
    "    if not src.exists():\n",
    "        vprint(f\"âš  Missing design file: {src}\")\n",
    "        return False\n",
    "    shutil.copy2(src, dst)\n",
    "    vprint(f\"â†’ Copied {src.name} -> rtl/\")\n",
    "    return True\n",
    "\n",
    "def resolve_and_copy(mod):\n",
    "    cand = [f\"{mod}.v\"]\n",
    "    if mod.lower() in (\"conv2d\", \"convch\", \"convolchnl\", \"convchnl\"):\n",
    "        cand += [\"Conv2D.v\", \"ConvolChnl.v\", \"ConvChnl.v\"]\n",
    "    found = False\n",
    "    for c in cand:\n",
    "        if (DESIGN_DIR / c).exists():\n",
    "            copy_if_exists(c)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        vprint(f\"âš  Could not locate module {mod} (tried {cand})\")\n",
    "    for dep in DEPENDENCY_BASE.get(mod, []):\n",
    "        copy_if_exists(dep)\n",
    "\n",
    "def ensure_keys(d, keys, ctx):\n",
    "    for k in keys:\n",
    "        if k not in d:\n",
    "            raise KeyError(f\"[{ctx}] Missing mandatory parameter: '{k}'\")\n",
    "\n",
    "def derive_prev_output_channels(top):\n",
    "    \"\"\"Find block before NN and derive its output channel count.\"\"\"\n",
    "    keys = list(top.keys())\n",
    "    try:\n",
    "        idx = keys.index(\"NN\")\n",
    "    except ValueError:\n",
    "        raise KeyError(\"NN block missing in JSON\")\n",
    "    for k in reversed(keys[:idx]):\n",
    "        blk = top[k]\n",
    "        if isinstance(blk, dict) and \"module\" in blk:\n",
    "            params = blk.get(\"parameters\", {})\n",
    "            if \"OUTPUT_CHANNELS\" in params:\n",
    "                return int(params[\"OUTPUT_CHANNELS\"])\n",
    "            if \"CHANNELS\" in params:\n",
    "                return int(params[\"CHANNELS\"])\n",
    "    raise KeyError(\"Could not derive output channels from previous block\")\n",
    "\n",
    "def find_block_before_nn(top):\n",
    "    keys = list(top.keys())\n",
    "    try:\n",
    "        idx = keys.index(\"NN\")\n",
    "    except ValueError:\n",
    "        raise KeyError(\"NN block missing in JSON\")\n",
    "    for k in reversed(keys[:idx]):\n",
    "        blk = top[k]\n",
    "        if isinstance(blk, dict) and \"module\" in blk:\n",
    "            return k, blk\n",
    "    raise KeyError(\"No valid block found before NN\")\n",
    "\n",
    "# ---------- Layer File Generator ----------\n",
    "def gen_layer_file(idx, layer, hw_params, first_layer_input_channels=None):\n",
    "    name = f\"Layer_{idx}\"\n",
    "    DW = int(hw_params[\"DATA_WIDTH\"])\n",
    "    FRAC = int(hw_params[\"FRACTION_SIZE\"])\n",
    "    weightIntWidth = DW - FRAC\n",
    "    SIGMOID_SIZE = hw_params.get(\"SIGMOID_SIZE\", 10)\n",
    "\n",
    "    if \"input_channels\" in layer:\n",
    "        in_ch = int(layer[\"input_channels\"])\n",
    "    elif idx == 1 and first_layer_input_channels:\n",
    "        in_ch = first_layer_input_channels\n",
    "    else:\n",
    "        in_ch = 1\n",
    "\n",
    "    NN = int(layer[\"num_neurons\"])\n",
    "    numW = int(layer[\"num_inputs\"])\n",
    "    act = layer.get(\"activation\", \"relu\")\n",
    "    wpat = layer.get(\"weight_file_pattern\", f\"w_{idx}_<neuron>.mif\")\n",
    "    bpat = layer.get(\"bias_file_pattern\", f\"b_{idx}_<neuron>.mif\")\n",
    "\n",
    "    lines = [\n",
    "        \"// =============================================================\",\n",
    "        f\"// {name} â€” {layer.get('name','layer')}, act={act}, neurons={NN}\",\n",
    "        \"// =============================================================\",\n",
    "        f\"module {name} #(\",\n",
    "        f\"    parameter NN = {NN},\",\n",
    "        f\"    parameter numWeight = {numW},\",\n",
    "        f\"    parameter dataWidth = {DW},\",\n",
    "        f\"    parameter layerNum = {idx},\",\n",
    "        f\"    parameter sigmoidSize = {SIGMOID_SIZE},\",\n",
    "        f\"    parameter weightIntWidth = {weightIntWidth},\",\n",
    "        f\"    parameter input_channels = {in_ch},\",\n",
    "        f\"    parameter actType = \\\"{act}\\\"\",\n",
    "        f\")(\",\n",
    "        f\"    input           clk,\",\n",
    "        f\"    input           rst,\",\n",
    "        f\"    input           weightValid,\",\n",
    "        f\"    input           biasValid,\",\n",
    "        f\"    input  [31:0]   weightValue,\",\n",
    "        f\"    input  [31:0]   biasValue,\",\n",
    "        f\"    input  [31:0]   config_layer_num,\",\n",
    "        f\"    input  [31:0]   config_neuron_num,\",\n",
    "        f\"    input           x_valid,\",\n",
    "        f\"    input  [input_channels*dataWidth-1:0] x_in,\",\n",
    "        f\"    output [NN-1:0] o_valid,\",\n",
    "        f\"    output [NN*dataWidth-1:0] x_out\",\n",
    "        f\");\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    for n in range(NN):\n",
    "        wfile = wpat.replace(\"<neuron>\", str(n))\n",
    "        bfile = bpat.replace(\"<neuron>\", str(n))\n",
    "        lines += [\n",
    "            f\"    neuron #(\",\n",
    "            f\"        .input_channels(input_channels),\",\n",
    "            f\"        .numWeight(numWeight),\",\n",
    "            f\"        .layerNo({idx}),\",\n",
    "            f\"        .neuronNo({n}),\",\n",
    "            f\"        .dataWidth(dataWidth),\",\n",
    "            f\"        .sigmoidSize(sigmoidSize),\",\n",
    "            f\"        .weightIntWidth(weightIntWidth),\",\n",
    "            f\"        .actType(actType),\",\n",
    "            f\"        .weightFile(\\\"{wfile}\\\"),\",\n",
    "            f\"        .biasFile(\\\"{bfile}\\\")\",\n",
    "            f\"    ) n_{n} (\",\n",
    "            f\"        .clk(clk),\",\n",
    "            f\"        .rst(rst),\",\n",
    "            f\"        .myinput(x_in),\",\n",
    "            f\"        .weightValid(weightValid),\",\n",
    "            f\"        .biasValid(biasValid),\",\n",
    "            f\"        .weightValue(weightValue),\",\n",
    "            f\"        .biasValue(biasValue),\",\n",
    "            f\"        .config_layer_num(config_layer_num),\",\n",
    "            f\"        .config_neuron_num(config_neuron_num),\",\n",
    "            f\"        .myinputValid(x_valid),\",\n",
    "            f\"        .out(x_out[{n}*dataWidth +: dataWidth]),\",\n",
    "            f\"        .outvalid(o_valid[{n}])\",\n",
    "            f\"    );\",\n",
    "            \"\"\n",
    "        ]\n",
    "\n",
    "    lines.append(\"endmodule\")\n",
    "    (RTL_DIR / f\"{name}.v\").write_text(\"\\n\".join(lines))\n",
    "    vprint(f\"âœ“ Wrote {name}.v\")\n",
    "\n",
    "# ---------- Top Generator ----------\n",
    "def gen_top(top):\n",
    "    hwp = top[\"hardware parameters\"]\n",
    "    ensure_keys(hwp, [\"DATA_WIDTH\",\"FRACTION_SIZE\",\"SIGNED\",\"GUARD_TYPE\"], \"hardware parameters\")\n",
    "    DW = int(hwp[\"DATA_WIDTH\"])\n",
    "    FRAC = int(hwp[\"FRACTION_SIZE\"])\n",
    "\n",
    "    lines = [\n",
    "        \"// =============================================================\",\n",
    "        \"// Auto-generated CNN_top (readable, JSON-driven)\",\n",
    "        \"// =============================================================\",\n",
    "        f\"module CNN_top #(\",\n",
    "        f\"    parameter DATA_WIDTH = {DW},\",\n",
    "        f\"    parameter FRACTION_SIZE = {hwp['FRACTION_SIZE']},\",\n",
    "        f\"    parameter SIGNED = {hwp['SIGNED']},\",\n",
    "        f\"    parameter GUARD_TYPE = {hwp['GUARD_TYPE']}\",\n",
    "        f\")(\",\n",
    "        f\"    input wire clock,\",\n",
    "        f\"    input wire sreset_n,\",\n",
    "        f\"    input wire [DATA_WIDTH-1:0] data_in,\",\n",
    "        f\"    input wire data_valid,\",\n",
    "        f\"    output wire [3:0] class_idx,\",\n",
    "        f\"    output wire class_valid\",\n",
    "        f\");\",\n",
    "        \"\",\n",
    "        \"    wire reset = ~sreset_n;\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    prev_block_name = None\n",
    "    prev_block = None\n",
    "    for k, blk in top.items():\n",
    "        if k in (\"hardware parameters\",\"inputs\",\"outputs\",\"module\",\"NN\",\"maxFinder\"):\n",
    "            continue\n",
    "        if not isinstance(blk, dict) or \"module\" not in blk:\n",
    "            continue\n",
    "\n",
    "        mod = blk[\"module\"]\n",
    "        params = blk.get(\"parameters\", {})\n",
    "        inputs = blk.get(\"inputs\", {})\n",
    "        outputs = blk.get(\"outputs\", {})\n",
    "\n",
    "        # ---------- Conv2D ----------\n",
    "        if mod.lower() == \"conv2d\":\n",
    "            required_conv = [\n",
    "                \"KERNEL_SIZE\",\"COLUMN_NUM\",\"ROW_NUM\",\"STRIDE\",\n",
    "                \"INPUT_CHANNELS\",\"OUTPUT_CHANNELS\",\"DATA_WIDTH\",\n",
    "                \"FRACTION_SIZE\",\"SIGNED\",\"ACTIVATION\",\"GUARD_TYPE\",\n",
    "                \"WEIGHT_FILE\",\"BIAS_FILE\"\n",
    "            ]\n",
    "            ensure_keys(params, required_conv, f\"{k} Conv2D\")\n",
    "            out_ch = int(params[\"OUTPUT_CHANNELS\"])\n",
    "\n",
    "            conv_out_sig = outputs.get(\"conv_out\", {}).get(\"name\", f\"{k}_out\")\n",
    "            conv_valid_sig = outputs.get(\"conv_valid\", {}).get(\"name\", f\"{k}_valid\")\n",
    "\n",
    "            lines += [\n",
    "                \"    // ------------------------------------------------------------\",\n",
    "                f\"    // {k}: Conv2D\",\n",
    "                \"    // ------------------------------------------------------------\",\n",
    "                f\"    wire [{out_ch}*DATA_WIDTH-1:0] {conv_out_sig};\",\n",
    "                f\"    wire {conv_valid_sig};\",\n",
    "                \"\",\n",
    "                f\"    Conv2D #(\"\n",
    "            ]\n",
    "            for pname, pval in params.items():\n",
    "                if pname in (\"WEIGHT_FILE\",\"BIAS_FILE\"):\n",
    "                    lines.append(f\"        .{pname}(\\\"{pval}\\\"),\")\n",
    "                else:\n",
    "                    lines.append(f\"        .{pname}({pval}),\")\n",
    "            lines[-1] = lines[-1].rstrip(',')\n",
    "            lines += [\n",
    "                f\"    ) {k} (\",\n",
    "                f\"        .clock(clock),\",\n",
    "                f\"        .sreset_n(sreset_n),\",\n",
    "                f\"        .data_valid({inputs.get('data_valid',{}).get('name','data_valid')}),\",\n",
    "                f\"        .data_in({inputs.get('data_in',{}).get('name','data_in')}),\",\n",
    "                f\"        .conv_out({conv_out_sig}),\",\n",
    "                f\"        .conv_valid({conv_valid_sig})\",\n",
    "                f\"    );\",\n",
    "                \"\"\n",
    "            ]\n",
    "            resolve_and_copy(\"Conv2D\")\n",
    "            prev_block_name, prev_block = k, blk\n",
    "\n",
    "        # ---------- Max2D ----------\n",
    "        elif mod.lower() == \"max2d\":\n",
    "            required_max = [\"KERNEL_SIZE\",\"DATA_WIDTH\",\"COLUMN_NUM\",\"ROW_NUM\",\"STRIDE\",\"CHANNELS\",\"SIGNED\"]\n",
    "            ensure_keys(params, required_max, f\"{k} Max2D\")\n",
    "            out_ch = int(params[\"CHANNELS\"])\n",
    "\n",
    "            max_out_sig = outputs.get(\"maxp_out\", {}).get(\"name\", f\"{k}_out\")\n",
    "            max_valid_sig = outputs.get(\"maxp_valid\", {}).get(\"name\", f\"{k}_valid\")\n",
    "\n",
    "            lines += [\n",
    "                \"    // ------------------------------------------------------------\",\n",
    "                f\"    // {k}: Max2D\",\n",
    "                \"    // ------------------------------------------------------------\",\n",
    "                f\"    wire [{out_ch}*DATA_WIDTH-1:0] {max_out_sig};\",\n",
    "                f\"    wire {max_valid_sig};\",\n",
    "                \"\",\n",
    "                f\"    Max2D #(\"\n",
    "            ]\n",
    "            for pname, pval in params.items():\n",
    "                lines.append(f\"        .{pname}({pval}),\")\n",
    "            lines[-1] = lines[-1].rstrip(',')\n",
    "            lines += [\n",
    "                f\"    ) {k} (\",\n",
    "                f\"        .clock(clock),\",\n",
    "                f\"        .sreset_n(sreset_n),\",\n",
    "                f\"        .data_valid({inputs.get('data_valid',{}).get('name','data_valid')}),\",\n",
    "                f\"        .data_in({inputs.get('data_in',{}).get('name','data_in')}),\",\n",
    "                f\"        .maxp_out({max_out_sig}),\",\n",
    "                f\"        .maxp_valid({max_valid_sig})\",\n",
    "                f\"    );\",\n",
    "                \"\"\n",
    "            ]\n",
    "            resolve_and_copy(\"Max2D\")\n",
    "            prev_block_name, prev_block = k, blk\n",
    "\n",
    "    # ----------- Derive connection to NN -----------\n",
    "    prev_outputs = prev_block[\"outputs\"]\n",
    "    prev_data_sig = list(prev_outputs.values())[0][\"name\"]\n",
    "    prev_valid_sig = list(prev_outputs.values())[1][\"name\"]\n",
    "\n",
    "    nn_block = top[\"NN\"]\n",
    "    layers = nn_block[\"layers\"]\n",
    "    first_in_ch = derive_prev_output_channels(top)\n",
    "\n",
    "    # ----------- Generate Layer_X.v files -----------\n",
    "    for i, layer in enumerate(layers, start=1):\n",
    "        gen_layer_file(i, layer, hwp, first_layer_input_channels=first_in_ch if i == 1 else None)\n",
    "        resolve_and_copy(\"neuron\")\n",
    "\n",
    "    # ----------- NN chaining + FSMs -----------\n",
    "    lines += [\n",
    "        \"    // ------------------------------------------------------------\",\n",
    "        \"    // Neural Network Layers (with IDLE/SEND FSMs between layers)\",\n",
    "        \"    // ------------------------------------------------------------\",\n",
    "        \"    localparam IDLE = 1'b0;\",\n",
    "        \"    localparam SEND = 1'b1;\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    prev_data = prev_data_sig\n",
    "    prev_valid = prev_valid_sig\n",
    "    prev_num_neurons = None\n",
    "    SIGMOID_SIZE = int(hwp.get(\"SIGMOID_SIZE\", 10))\n",
    "    weightIntWidth = DW - FRAC\n",
    "\n",
    "    for i, layer in enumerate(layers, start=1):\n",
    "        NN = int(layer[\"num_neurons\"])\n",
    "        numWeight = int(layer[\"num_inputs\"])\n",
    "        act = layer.get(\"activation\", \"relu\")\n",
    "        in_ch = int(layer.get(\"input_channels\", prev_num_neurons or first_in_ch))\n",
    "        prev_num_neurons = NN\n",
    "\n",
    "        lines += [\n",
    "            f\"    // ------------------------------------------------------------\",\n",
    "            f\"    // Layer {i} â€” {layer.get('name','layer')} ({act.upper()}, {NN} neurons)\",\n",
    "            f\"    // ------------------------------------------------------------\",\n",
    "            f\"    wire [{NN-1}:0] o{i}_valid;\",\n",
    "            f\"    wire [{NN}*DATA_WIDTH-1:0] x{i}_out;\",\n",
    "            f\"    reg  [{NN}*DATA_WIDTH-1:0] holdData_{i};\",\n",
    "            f\"    reg  [DATA_WIDTH-1:0] out_data_{i};\",\n",
    "            f\"    reg  data_out_valid_{i};\",\n",
    "            \"\",\n",
    "            f\"    Layer_{i} #(\",\n",
    "            f\"        .NN({NN}),\",\n",
    "            f\"        .numWeight({numWeight}),\",\n",
    "            f\"        .dataWidth(DATA_WIDTH),\",\n",
    "            f\"        .layerNum({i}),\",\n",
    "            f\"        .sigmoidSize({SIGMOID_SIZE}),\",\n",
    "            f\"        .weightIntWidth({weightIntWidth}),\",\n",
    "            f\"        .input_channels({in_ch}),\",\n",
    "            f\"        .actType(\\\"{act}\\\")\",\n",
    "            f\"    ) L{i} (\",\n",
    "            f\"        .clk(clock),\",\n",
    "            f\"        .rst(reset),\",\n",
    "            f\"        .weightValid(weightValid),\",\n",
    "            f\"        .biasValid(biasValid),\",\n",
    "            f\"        .weightValue(weightValue),\",\n",
    "            f\"        .biasValue(biasValue),\",\n",
    "            f\"        .config_layer_num(config_layer_num),\",\n",
    "            f\"        .config_neuron_num(config_neuron_num),\",\n",
    "            f\"        .x_valid({prev_valid}),\",\n",
    "            f\"        .x_in({prev_data}),\",\n",
    "            f\"        .o_valid(o{i}_valid),\",\n",
    "            f\"        .x_out(x{i}_out)\",\n",
    "            f\"    );\",\n",
    "            \"\",\n",
    "            f\"    reg state_{i}; integer count_{i};\",\n",
    "            f\"    always @(posedge clock) begin\",\n",
    "            f\"        if (reset) begin\",\n",
    "            f\"            state_{i} <= IDLE; count_{i} <= 0; data_out_valid_{i} <= 0;\",\n",
    "            f\"        end else case (state_{i})\",\n",
    "            f\"            IDLE: begin\",\n",
    "            f\"                count_{i} <= 0; data_out_valid_{i} <= 0;\",\n",
    "            f\"                if (o{i}_valid[0]) begin holdData_{i} <= x{i}_out; state_{i} <= SEND; end\",\n",
    "            f\"            end\",\n",
    "            f\"            SEND: begin\",\n",
    "            f\"                out_data_{i} <= holdData_{i}[DATA_WIDTH-1:0];\",\n",
    "            f\"                holdData_{i} <= holdData_{i} >> DATA_WIDTH;\",\n",
    "            f\"                count_{i} <= count_{i}+1; data_out_valid_{i} <= 1;\",\n",
    "            f\"                if (count_{i} == {NN}) begin state_{i} <= IDLE; data_out_valid_{i} <= 0; end\",\n",
    "            f\"            end\",\n",
    "            f\"        endcase\",\n",
    "            f\"    end\",\n",
    "            \"\"\n",
    "        ]\n",
    "\n",
    "        prev_data = f\"out_data_{i}\"\n",
    "        prev_valid = f\"data_out_valid_{i}\"\n",
    "\n",
    "    # ----------- Final Layer + maxFinder -----------\n",
    "    last_layer = layers[-1]\n",
    "    last_NN = last_layer[\"num_neurons\"]\n",
    "    last_act = last_layer[\"activation\"].lower()\n",
    "\n",
    "    if last_act in (\"softmax\",\"hardmax\"):\n",
    "        lines += [\n",
    "            \"    // ------------------------------------------------------------\",\n",
    "            \"    // Final Layer + maxFinder\",\n",
    "            \"    // ------------------------------------------------------------\",\n",
    "            f\"    reg [{last_NN}*DATA_WIDTH-1:0] holdData_final;\",\n",
    "            f\"    always @(posedge clock) begin\",\n",
    "            f\"        if (o{len(layers)}_valid[0]) holdData_final <= x{len(layers)}_out;\",\n",
    "            f\"    end\",\n",
    "            \"\",\n",
    "            f\"    maxFinder #(\",\n",
    "            f\"        .NUM_INPUTS({last_NN}),\",\n",
    "            f\"        .DATA_WIDTH(DATA_WIDTH),\",\n",
    "            f\"        .SIGNED(SIGNED)\",\n",
    "            f\"    ) mFind (\",\n",
    "            f\"        .i_clk(clock),\",\n",
    "            f\"        .i_data(x{len(layers)}_out),\",\n",
    "            f\"        .i_valid(o{len(layers)}_valid),\",\n",
    "            f\"        .o_data(class_idx),\",\n",
    "            f\"        .o_data_valid(class_valid)\",\n",
    "            f\"    );\",\n",
    "            \"\"\n",
    "        ]\n",
    "        resolve_and_copy(\"maxFinder\")\n",
    "\n",
    "    lines.append(\"endmodule\")\n",
    "    (RTL_DIR / \"CNN_top.v\").write_text(\"\\n\".join(lines))\n",
    "    vprint(\"âœ“ Wrote rtl/CNN_top.v\")\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    ensure_dirs()\n",
    "    top = read_json()\n",
    "    for mod in [\"Conv2D\", \"Max2D\", \"neuron\", \"maxFinder\"]:\n",
    "        resolve_and_copy(mod)\n",
    "    gen_top(top)\n",
    "    vprint(\"\\nâœ… build_rtl_v5 complete. Inspect rtl/*.v\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17048b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Cleaning existing /hw folder ...\n",
      "ğŸ“¦ Packaging RTL and MIF files into hw/\n",
      "â†’ Copied CNN_top.v\n",
      "â†’ Copied Conv2D.v\n",
      "â†’ Copied ConvMemory.v\n",
      "â†’ Copied ConvolChnl.v\n",
      "â†’ Copied Conv_MIC.v\n",
      "â†’ Copied Conv_SIC.v\n",
      "â†’ Copied FP_Adder.v\n",
      "â†’ Copied FP_Comparator.v\n",
      "â†’ Copied FP_Multiplier.v\n",
      "â†’ Copied ImageBuffer.v\n",
      "â†’ Copied ImageBufferChnl.v\n",
      "â†’ Copied Layer_1.v\n",
      "â†’ Copied Layer_2.v\n",
      "â†’ Copied Layer_3.v\n",
      "â†’ Copied line_buffer.v\n",
      "â†’ Copied Max2D.v\n",
      "â†’ Copied maxFinder.v\n",
      "â†’ Copied Maxpool.v\n",
      "â†’ Copied MaxpoolChnl.v\n",
      "â†’ Copied neuron.v\n",
      "â†’ Copied Pixel_Buffer.v\n",
      "â†’ Copied relu.v\n",
      "â†’ Copied Sig_ROM.v\n",
      "â†’ Copied Weight_Memory.v\n",
      "â†’ Copied b_1_0.mif\n",
      "â†’ Copied b_1_1.mif\n",
      "â†’ Copied b_1_10.mif\n",
      "â†’ Copied b_1_11.mif\n",
      "â†’ Copied b_1_12.mif\n",
      "â†’ Copied b_1_13.mif\n",
      "â†’ Copied b_1_14.mif\n",
      "â†’ Copied b_1_15.mif\n",
      "â†’ Copied b_1_16.mif\n",
      "â†’ Copied b_1_17.mif\n",
      "â†’ Copied b_1_18.mif\n",
      "â†’ Copied b_1_19.mif\n",
      "â†’ Copied b_1_2.mif\n",
      "â†’ Copied b_1_20.mif\n",
      "â†’ Copied b_1_21.mif\n",
      "â†’ Copied b_1_22.mif\n",
      "â†’ Copied b_1_23.mif\n",
      "â†’ Copied b_1_24.mif\n",
      "â†’ Copied b_1_25.mif\n",
      "â†’ Copied b_1_26.mif\n",
      "â†’ Copied b_1_27.mif\n",
      "â†’ Copied b_1_28.mif\n",
      "â†’ Copied b_1_29.mif\n",
      "â†’ Copied b_1_3.mif\n",
      "â†’ Copied b_1_30.mif\n",
      "â†’ Copied b_1_31.mif\n",
      "â†’ Copied b_1_32.mif\n",
      "â†’ Copied b_1_33.mif\n",
      "â†’ Copied b_1_34.mif\n",
      "â†’ Copied b_1_35.mif\n",
      "â†’ Copied b_1_36.mif\n",
      "â†’ Copied b_1_37.mif\n",
      "â†’ Copied b_1_38.mif\n",
      "â†’ Copied b_1_39.mif\n",
      "â†’ Copied b_1_4.mif\n",
      "â†’ Copied b_1_40.mif\n",
      "â†’ Copied b_1_41.mif\n",
      "â†’ Copied b_1_42.mif\n",
      "â†’ Copied b_1_43.mif\n",
      "â†’ Copied b_1_44.mif\n",
      "â†’ Copied b_1_45.mif\n",
      "â†’ Copied b_1_46.mif\n",
      "â†’ Copied b_1_47.mif\n",
      "â†’ Copied b_1_48.mif\n",
      "â†’ Copied b_1_49.mif\n",
      "â†’ Copied b_1_5.mif\n",
      "â†’ Copied b_1_50.mif\n",
      "â†’ Copied b_1_51.mif\n",
      "â†’ Copied b_1_52.mif\n",
      "â†’ Copied b_1_53.mif\n",
      "â†’ Copied b_1_54.mif\n",
      "â†’ Copied b_1_55.mif\n",
      "â†’ Copied b_1_56.mif\n",
      "â†’ Copied b_1_57.mif\n",
      "â†’ Copied b_1_58.mif\n",
      "â†’ Copied b_1_59.mif\n",
      "â†’ Copied b_1_6.mif\n",
      "â†’ Copied b_1_60.mif\n",
      "â†’ Copied b_1_61.mif\n",
      "â†’ Copied b_1_62.mif\n",
      "â†’ Copied b_1_63.mif\n",
      "â†’ Copied b_1_7.mif\n",
      "â†’ Copied b_1_8.mif\n",
      "â†’ Copied b_1_9.mif\n",
      "â†’ Copied b_2_0.mif\n",
      "â†’ Copied b_2_1.mif\n",
      "â†’ Copied b_2_10.mif\n",
      "â†’ Copied b_2_11.mif\n",
      "â†’ Copied b_2_12.mif\n",
      "â†’ Copied b_2_13.mif\n",
      "â†’ Copied b_2_14.mif\n",
      "â†’ Copied b_2_15.mif\n",
      "â†’ Copied b_2_16.mif\n",
      "â†’ Copied b_2_17.mif\n",
      "â†’ Copied b_2_18.mif\n",
      "â†’ Copied b_2_19.mif\n",
      "â†’ Copied b_2_2.mif\n",
      "â†’ Copied b_2_20.mif\n",
      "â†’ Copied b_2_21.mif\n",
      "â†’ Copied b_2_22.mif\n",
      "â†’ Copied b_2_23.mif\n",
      "â†’ Copied b_2_24.mif\n",
      "â†’ Copied b_2_25.mif\n",
      "â†’ Copied b_2_26.mif\n",
      "â†’ Copied b_2_27.mif\n",
      "â†’ Copied b_2_28.mif\n",
      "â†’ Copied b_2_29.mif\n",
      "â†’ Copied b_2_3.mif\n",
      "â†’ Copied b_2_30.mif\n",
      "â†’ Copied b_2_31.mif\n",
      "â†’ Copied b_2_4.mif\n",
      "â†’ Copied b_2_5.mif\n",
      "â†’ Copied b_2_6.mif\n",
      "â†’ Copied b_2_7.mif\n",
      "â†’ Copied b_2_8.mif\n",
      "â†’ Copied b_2_9.mif\n",
      "â†’ Copied b_3_0.mif\n",
      "â†’ Copied b_3_1.mif\n",
      "â†’ Copied b_3_2.mif\n",
      "â†’ Copied b_3_3.mif\n",
      "â†’ Copied b_3_4.mif\n",
      "â†’ Copied b_3_5.mif\n",
      "â†’ Copied b_3_6.mif\n",
      "â†’ Copied b_3_7.mif\n",
      "â†’ Copied b_3_8.mif\n",
      "â†’ Copied b_3_9.mif\n",
      "â†’ Copied c_1_b.mif\n",
      "â†’ Copied c_1_w.mif\n",
      "â†’ Copied w_1_0.mif\n",
      "â†’ Copied w_1_1.mif\n",
      "â†’ Copied w_1_10.mif\n",
      "â†’ Copied w_1_11.mif\n",
      "â†’ Copied w_1_12.mif\n",
      "â†’ Copied w_1_13.mif\n",
      "â†’ Copied w_1_14.mif\n",
      "â†’ Copied w_1_15.mif\n",
      "â†’ Copied w_1_16.mif\n",
      "â†’ Copied w_1_17.mif\n",
      "â†’ Copied w_1_18.mif\n",
      "â†’ Copied w_1_19.mif\n",
      "â†’ Copied w_1_2.mif\n",
      "â†’ Copied w_1_20.mif\n",
      "â†’ Copied w_1_21.mif\n",
      "â†’ Copied w_1_22.mif\n",
      "â†’ Copied w_1_23.mif\n",
      "â†’ Copied w_1_24.mif\n",
      "â†’ Copied w_1_25.mif\n",
      "â†’ Copied w_1_26.mif\n",
      "â†’ Copied w_1_27.mif\n",
      "â†’ Copied w_1_28.mif\n",
      "â†’ Copied w_1_29.mif\n",
      "â†’ Copied w_1_3.mif\n",
      "â†’ Copied w_1_30.mif\n",
      "â†’ Copied w_1_31.mif\n",
      "â†’ Copied w_1_32.mif\n",
      "â†’ Copied w_1_33.mif\n",
      "â†’ Copied w_1_34.mif\n",
      "â†’ Copied w_1_35.mif\n",
      "â†’ Copied w_1_36.mif\n",
      "â†’ Copied w_1_37.mif\n",
      "â†’ Copied w_1_38.mif\n",
      "â†’ Copied w_1_39.mif\n",
      "â†’ Copied w_1_4.mif\n",
      "â†’ Copied w_1_40.mif\n",
      "â†’ Copied w_1_41.mif\n",
      "â†’ Copied w_1_42.mif\n",
      "â†’ Copied w_1_43.mif\n",
      "â†’ Copied w_1_44.mif\n",
      "â†’ Copied w_1_45.mif\n",
      "â†’ Copied w_1_46.mif\n",
      "â†’ Copied w_1_47.mif\n",
      "â†’ Copied w_1_48.mif\n",
      "â†’ Copied w_1_49.mif\n",
      "â†’ Copied w_1_5.mif\n",
      "â†’ Copied w_1_50.mif\n",
      "â†’ Copied w_1_51.mif\n",
      "â†’ Copied w_1_52.mif\n",
      "â†’ Copied w_1_53.mif\n",
      "â†’ Copied w_1_54.mif\n",
      "â†’ Copied w_1_55.mif\n",
      "â†’ Copied w_1_56.mif\n",
      "â†’ Copied w_1_57.mif\n",
      "â†’ Copied w_1_58.mif\n",
      "â†’ Copied w_1_59.mif\n",
      "â†’ Copied w_1_6.mif\n",
      "â†’ Copied w_1_60.mif\n",
      "â†’ Copied w_1_61.mif\n",
      "â†’ Copied w_1_62.mif\n",
      "â†’ Copied w_1_63.mif\n",
      "â†’ Copied w_1_7.mif\n",
      "â†’ Copied w_1_8.mif\n",
      "â†’ Copied w_1_9.mif\n",
      "â†’ Copied w_2_0.mif\n",
      "â†’ Copied w_2_1.mif\n",
      "â†’ Copied w_2_10.mif\n",
      "â†’ Copied w_2_11.mif\n",
      "â†’ Copied w_2_12.mif\n",
      "â†’ Copied w_2_13.mif\n",
      "â†’ Copied w_2_14.mif\n",
      "â†’ Copied w_2_15.mif\n",
      "â†’ Copied w_2_16.mif\n",
      "â†’ Copied w_2_17.mif\n",
      "â†’ Copied w_2_18.mif\n",
      "â†’ Copied w_2_19.mif\n",
      "â†’ Copied w_2_2.mif\n",
      "â†’ Copied w_2_20.mif\n",
      "â†’ Copied w_2_21.mif\n",
      "â†’ Copied w_2_22.mif\n",
      "â†’ Copied w_2_23.mif\n",
      "â†’ Copied w_2_24.mif\n",
      "â†’ Copied w_2_25.mif\n",
      "â†’ Copied w_2_26.mif\n",
      "â†’ Copied w_2_27.mif\n",
      "â†’ Copied w_2_28.mif\n",
      "â†’ Copied w_2_29.mif\n",
      "â†’ Copied w_2_3.mif\n",
      "â†’ Copied w_2_30.mif\n",
      "â†’ Copied w_2_31.mif\n",
      "â†’ Copied w_2_4.mif\n",
      "â†’ Copied w_2_5.mif\n",
      "â†’ Copied w_2_6.mif\n",
      "â†’ Copied w_2_7.mif\n",
      "â†’ Copied w_2_8.mif\n",
      "â†’ Copied w_2_9.mif\n",
      "â†’ Copied w_3_0.mif\n",
      "â†’ Copied w_3_1.mif\n",
      "â†’ Copied w_3_2.mif\n",
      "â†’ Copied w_3_3.mif\n",
      "â†’ Copied w_3_4.mif\n",
      "â†’ Copied w_3_5.mif\n",
      "â†’ Copied w_3_6.mif\n",
      "â†’ Copied w_3_7.mif\n",
      "â†’ Copied w_3_8.mif\n",
      "â†’ Copied w_3_9.mif\n",
      "\n",
      "âœ… Packaging complete â€” /hw now contains RTL + MIF files only.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "package_hw.py\n",
    "\n",
    "Creates a clean /hw folder and copies:\n",
    " - Verilog RTL files (*.v) from /rtl\n",
    " - Memory initialization files (*.mif) from /Memory_Files/mif\n",
    "Excludes all JSON and non-relevant files.\n",
    "\"\"\"\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Folder paths ---\n",
    "RTL_DIR = Path(\"rtl\")\n",
    "MIF_DIR = Path(\"Memory_Files/mif\")\n",
    "HW_DIR = Path(\"hw\")\n",
    "\n",
    "def clean_hw_folder():\n",
    "    \"\"\"Completely remove and recreate /hw.\"\"\"\n",
    "    if HW_DIR.exists():\n",
    "        print(\"ğŸ§¹ Cleaning existing /hw folder ...\")\n",
    "        shutil.rmtree(HW_DIR)\n",
    "    HW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def copy_files(src_dir: Path, dst_dir: Path, extensions):\n",
    "    \"\"\"Copy files with specific extensions from src_dir to dst_dir.\"\"\"\n",
    "    if not src_dir.exists():\n",
    "        print(f\"âš  Skipping missing folder: {src_dir}\")\n",
    "        return\n",
    "    for f in src_dir.iterdir():\n",
    "        if f.is_file() and f.suffix.lower() in extensions:\n",
    "            shutil.copy2(f, dst_dir / f.name)\n",
    "            print(f\"â†’ Copied {f.name}\")\n",
    "\n",
    "def package_hw():\n",
    "    clean_hw_folder()\n",
    "    print(f\"ğŸ“¦ Packaging RTL and MIF files into {HW_DIR}/\")\n",
    "\n",
    "    # Copy Verilog RTL files\n",
    "    copy_files(RTL_DIR, HW_DIR, {\".v\"})\n",
    "\n",
    "    # Copy .mif files from Memory_Files/mif/\n",
    "    copy_files(MIF_DIR, HW_DIR, {\".mif\"})\n",
    "\n",
    "    print(\"\\nâœ… Packaging complete â€” /hw now contains RTL + MIF files only.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    package_hw()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
